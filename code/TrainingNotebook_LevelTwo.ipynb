{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-30T07:52:44.567027Z",
     "iopub.status.busy": "2020-12-30T07:52:44.566305Z",
     "iopub.status.idle": "2020-12-30T07:52:56.733294Z",
     "shell.execute_reply": "2020-12-30T07:52:56.732253Z"
    },
    "papermill": {
     "duration": 12.203546,
     "end_time": "2020-12-30T07:52:56.733417",
     "exception": false,
     "start_time": "2020-12-30T07:52:44.529871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import datetime\n",
    "from scipy.optimize import minimize, fsolve\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import QuantileTransformer, RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "\n",
    "import copy\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-30T07:52:56.788888Z",
     "iopub.status.busy": "2020-12-30T07:52:56.788148Z",
     "iopub.status.idle": "2020-12-30T07:52:56.795509Z",
     "shell.execute_reply": "2020-12-30T07:52:56.794993Z"
    },
    "papermill": {
     "duration": 0.0378,
     "end_time": "2020-12-30T07:52:56.795630",
     "exception": false,
     "start_time": "2020-12-30T07:52:56.757830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED=42\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024013,
     "end_time": "2020-12-30T07:52:56.844066",
     "exception": false,
     "start_time": "2020-12-30T07:52:56.820053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:52:56.899673Z",
     "iopub.status.busy": "2020-12-30T07:52:56.899002Z",
     "iopub.status.idle": "2020-12-30T07:53:03.162314Z",
     "shell.execute_reply": "2020-12-30T07:53:03.161144Z"
    },
    "papermill": {
     "duration": 6.294105,
     "end_time": "2020-12-30T07:53:03.162438",
     "exception": false,
     "start_time": "2020-12-30T07:52:56.868333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_drug_ids = pd.read_csv('../input/lish-moa/train_drug.csv') \n",
    "\n",
    "train_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "\n",
    "train_data = train_features.merge(train_targets, on='sig_id', how='left')\n",
    "train_data = train_data.merge(train_drug_ids, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:03.222129Z",
     "iopub.status.busy": "2020-12-30T07:53:03.221477Z",
     "iopub.status.idle": "2020-12-30T07:53:12.651065Z",
     "shell.execute_reply": "2020-12-30T07:53:12.650435Z"
    },
    "papermill": {
     "duration": 9.464594,
     "end_time": "2020-12-30T07:53:12.651187",
     "exception": false,
     "start_time": "2020-12-30T07:53:03.186593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_MODELS = 4\n",
    "\n",
    "meta_features = pd.DataFrame(data=train_data.sig_id.values, columns=['sig_id'])\n",
    "\n",
    "for version in range(1, NUMBER_OF_MODELS + 1):\n",
    "    df = pd.read_csv(f'../input/moablogstackingdataset/version_{version}.csv')\n",
    "    df = df.rename(columns={v:f'meta-f-{i}-model{version}' if v != 'sig_id' else v for i, v in enumerate(df.columns)})\n",
    "    meta_features = pd.merge(meta_features, df, on='sig_id')\n",
    "    \n",
    "train_data = pd.merge(train_data, meta_features, on='sig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:12.705225Z",
     "iopub.status.busy": "2020-12-30T07:53:12.703805Z",
     "iopub.status.idle": "2020-12-30T07:53:12.706339Z",
     "shell.execute_reply": "2020-12-30T07:53:12.706830Z"
    },
    "papermill": {
     "duration": 0.030725,
     "end_time": "2020-12-30T07:53:12.706947",
     "exception": false,
     "start_time": "2020-12-30T07:53:12.676222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# artificial_meta_features = pd.DataFrame(data=np.zeros((len(train_data), 206 * 4)), columns=[f'meta-f{i}' for i in range(206 * 4)])\n",
    "# artificial_meta_features['sig_id'] = train_data.sig_id.values\n",
    "# train_data = pd.merge(train_data, artificial_meta_features, on='sig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:12.762812Z",
     "iopub.status.busy": "2020-12-30T07:53:12.761362Z",
     "iopub.status.idle": "2020-12-30T07:53:12.763907Z",
     "shell.execute_reply": "2020-12-30T07:53:12.764370Z"
    },
    "papermill": {
     "duration": 0.032471,
     "end_time": "2020-12-30T07:53:12.764491",
     "exception": false,
     "start_time": "2020-12-30T07:53:12.732020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_columns = [c for c in train_targets.columns if c != 'sig_id']\n",
    "feature_columns = [c for c in train_data.columns if 'meta-f-' in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058169,
     "end_time": "2020-12-30T07:53:12.846577",
     "exception": false,
     "start_time": "2020-12-30T07:53:12.788408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cross validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:12.912211Z",
     "iopub.status.busy": "2020-12-30T07:53:12.910325Z",
     "iopub.status.idle": "2020-12-30T07:53:12.912883Z",
     "shell.execute_reply": "2020-12-30T07:53:12.913364Z"
    },
    "papermill": {
     "duration": 0.041784,
     "end_time": "2020-12-30T07:53:12.913516",
     "exception": false,
     "start_time": "2020-12-30T07:53:12.871732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_cross_validation_strategy(data, targets, FOLDS, SEED):\n",
    "\n",
    "    vc = data.drug_id.value_counts()\n",
    "    \n",
    "#     vc1 = vc.loc[(vc==6)|(vc==12)|(vc==18)].index.sort_values()\n",
    "#     vc2 = vc.loc[(vc!=6)&(vc!=12)&(vc!=18)].index.sort_values()\n",
    "    \n",
    "    vc1 = vc.loc[vc <= 19].index.sort_values()\n",
    "    vc2 = vc.loc[vc > 19].index.sort_values()\n",
    "\n",
    "    dct1 = {} \n",
    "    dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    tmp = data.groupby('drug_id')[targets].mean().loc[vc1]\n",
    "    \n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    tmp = data.loc[data.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "    \n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    # ASSIGN FOLDS\n",
    "    data['fold'] = data.drug_id.map(dct1)\n",
    "    data.loc[data.fold.isna(),'fold'] = data.loc[data.fold.isna(),'sig_id'].map(dct2)\n",
    "    data.fold = data.fold.astype('int8')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024664,
     "end_time": "2020-12-30T07:53:12.963463",
     "exception": false,
     "start_time": "2020-12-30T07:53:12.938799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.027579Z",
     "iopub.status.busy": "2020-12-30T07:53:13.026883Z",
     "iopub.status.idle": "2020-12-30T07:53:13.030553Z",
     "shell.execute_reply": "2020-12-30T07:53:13.031131Z"
    },
    "papermill": {
     "duration": 0.041734,
     "end_time": "2020-12-30T07:53:13.031264",
     "exception": false,
     "start_time": "2020-12-30T07:53:12.989530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoaMetaDataset:\n",
    "    def __init__(self, dataset_df, feature_ids, target_ids):\n",
    "        self.dataset_df = dataset_df\n",
    "        self.feature_ids = feature_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.num_models = len(feature_ids) // 206\n",
    "\n",
    "        # samples x models x targets\n",
    "        self.features = self.dataset_df[feature_ids].values\n",
    "        self.targets = None\n",
    "\n",
    "        if self.target_ids is not None:\n",
    "            self.targets = self.dataset_df[target_ids].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "\n",
    "    def num_of_features(self):\n",
    "        return len(feature_ids)\n",
    "\n",
    "    def num_of_targets(self):\n",
    "        return None if self.target_ids is None else len(self.target_ids)\n",
    "\n",
    "    def get_ids(self):\n",
    "        return self.dataset_df.sig_id.values\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return_item = {}\n",
    "        \n",
    "        return_item['x'] = torch.tensor(self.features[item, :].reshape(self.num_models, 206), dtype=torch.float)\n",
    "        return_item['sig_id'] = self.dataset_df.loc[item, 'sig_id']\n",
    "\n",
    "        if self.target_ids is not None:\n",
    "            return_item['y'] = torch.tensor(self.targets[item, :], dtype=torch.float)\n",
    "\n",
    "        return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.088285Z",
     "iopub.status.busy": "2020-12-30T07:53:13.086375Z",
     "iopub.status.idle": "2020-12-30T07:53:13.088980Z",
     "shell.execute_reply": "2020-12-30T07:53:13.089449Z"
    },
    "papermill": {
     "duration": 0.033325,
     "end_time": "2020-12-30T07:53:13.089568",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.056243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, number_of_features, number_of_targets):\n",
    "        self.number_of_features = number_of_features\n",
    "        self.number_of_targets = number_of_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.158825Z",
     "iopub.status.busy": "2020-12-30T07:53:13.156879Z",
     "iopub.status.idle": "2020-12-30T07:53:13.159539Z",
     "shell.execute_reply": "2020-12-30T07:53:13.160035Z"
    },
    "papermill": {
     "duration": 0.045539,
     "end_time": "2020-12-30T07:53:13.160163",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.114624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoaModelBlock(nn.Module):\n",
    "    def __init__(self, num_in, num_out, dropout, weight_norm=False):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(num_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if weight_norm:\n",
    "            self.linear = nn.utils.weight_norm(nn.Linear(num_in, num_out))\n",
    "        else:\n",
    "            self.linear = nn.Linear(num_in, num_out)\n",
    "        \n",
    "        self.activation = nn.PReLU(num_out)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class MoaEncodeBlock(nn.Module):\n",
    "    def __init__(self, num_in, num_out, dropout, weight_norm=False):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(num_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if weight_norm:\n",
    "            self.linear = nn.utils.weight_norm(nn.Linear(num_in, num_out))\n",
    "        else:\n",
    "            self.linear = nn.Linear(num_in, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.224540Z",
     "iopub.status.busy": "2020-12-30T07:53:13.223817Z",
     "iopub.status.idle": "2020-12-30T07:53:13.227560Z",
     "shell.execute_reply": "2020-12-30T07:53:13.227029Z"
    },
    "papermill": {
     "duration": 0.041072,
     "end_time": "2020-12-30T07:53:13.227669",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.186597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        self.num_models = model_config.number_of_features // model_config.number_of_targets\n",
    "        self.model_config = model_config\n",
    "        \n",
    "        dropout = 0.15\n",
    "        hidden_size = 512\n",
    "        \n",
    "        self.encoders = nn.ModuleList([MoaEncodeBlock(model_config.number_of_targets, 64, dropout) for i in range(self.num_models)])\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(64, hidden_size),\n",
    "                                   nn.Dropout(dropout),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_size, hidden_size),\n",
    "                                   nn.Dropout(dropout),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_size, model_config.number_of_targets))\n",
    "        \n",
    "\n",
    "    def forward(self, x): # batch size x models x features\n",
    "        x_ = self.encoders[0](x[:, 0, :])\n",
    "        for i in range(1, self.num_models):\n",
    "            x_ = x_ + self.encoders[i](x[:, i, :]) \n",
    "        return self.model(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025253,
     "end_time": "2020-12-30T07:53:13.278889",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.253636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Smooth loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.343395Z",
     "iopub.status.busy": "2020-12-30T07:53:13.342523Z",
     "iopub.status.idle": "2020-12-30T07:53:13.346708Z",
     "shell.execute_reply": "2020-12-30T07:53:13.346186Z"
    },
    "papermill": {
     "duration": 0.042177,
     "end_time": "2020-12-30T07:53:13.346839",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.304662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(DEVICE) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026932,
     "end_time": "2020-12-30T07:53:13.401005",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.374073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scaling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.465592Z",
     "iopub.status.busy": "2020-12-30T07:53:13.463561Z",
     "iopub.status.idle": "2020-12-30T07:53:13.466485Z",
     "shell.execute_reply": "2020-12-30T07:53:13.467078Z"
    },
    "papermill": {
     "duration": 0.038204,
     "end_time": "2020-12-30T07:53:13.467217",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.429013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantile_scaling(train_data, valid_data, feature_columns):\n",
    "    global RANDOM_SEED\n",
    "    \n",
    "    scaler = QuantileTransformer(n_quantiles=100,random_state=RANDOM_SEED, output_distribution=\"normal\")\n",
    "    train_data[feature_columns] = scaler.fit_transform(train_data[feature_columns])\n",
    "    valid_data[feature_columns] = scaler.transform(valid_data[feature_columns])\n",
    "\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028889,
     "end_time": "2020-12-30T07:53:13.524809",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.495920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.587395Z",
     "iopub.status.busy": "2020-12-30T07:53:13.586687Z",
     "iopub.status.idle": "2020-12-30T07:53:13.589845Z",
     "shell.execute_reply": "2020-12-30T07:53:13.590325Z"
    },
    "papermill": {
     "duration": 0.037474,
     "end_time": "2020-12-30T07:53:13.590451",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.552977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloader(data, batch_size, shuffle, target_columns=None):\n",
    "    global feature_columns\n",
    "    \n",
    "    dataset = MoaMetaDataset(data, feature_ids=feature_columns, target_ids=target_columns)\n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       batch_size=batch_size,                                    \n",
    "                                       shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.655248Z",
     "iopub.status.busy": "2020-12-30T07:53:13.654331Z",
     "iopub.status.idle": "2020-12-30T07:53:13.657162Z",
     "shell.execute_reply": "2020-12-30T07:53:13.657623Z"
    },
    "papermill": {
     "duration": 0.03977,
     "end_time": "2020-12-30T07:53:13.657777",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.618007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_fold_data(train_data, fold, scaling_func=None):\n",
    "    global feature_columns, target_columns, gene_features, cell_features\n",
    "    \n",
    "    fold_train_data = train_data[train_data.fold != fold].reset_index(drop=True)\n",
    "    fold_valid_data = train_data[train_data.fold == fold].reset_index(drop=True)\n",
    "    \n",
    "    if scaling_func is not None:\n",
    "        fold_train_data, fold_valid_data = scaling_func(fold_train_data, fold_valid_data, feature_columns)\n",
    "      \n",
    "    train_dataloader = create_dataloader(data=fold_train_data, batch_size=BATCH_SIZE, shuffle=True, target_columns=target_columns)\n",
    "    valid_dataloader = create_dataloader(data=fold_valid_data, batch_size=BATCH_SIZE, shuffle=False, target_columns=target_columns)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02574,
     "end_time": "2020-12-30T07:53:13.710535",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.684795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Blending functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.788396Z",
     "iopub.status.busy": "2020-12-30T07:53:13.787590Z",
     "iopub.status.idle": "2020-12-30T07:53:13.790977Z",
     "shell.execute_reply": "2020-12-30T07:53:13.791466Z"
    },
    "papermill": {
     "duration": 0.055171,
     "end_time": "2020-12-30T07:53:13.791592",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.736421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_numpy(y_pred):\n",
    "    loss = 0\n",
    "    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n",
    "    return loss / y_pred.shape[1]\n",
    "\n",
    "def func_numpy_metric(weights):\n",
    "    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n",
    "    score = log_loss_numpy(oof_blend)\n",
    "    \n",
    "    coef = 1e-6\n",
    "    penalty = coef * (np.sum(weights) - 1) ** 2\n",
    "    return score + penalty\n",
    "\n",
    "def grad_func(weights):\n",
    "    oof_clip = np.clip(oof, 1e-15, 1 - 1e-15)\n",
    "    gradients = np.zeros(oof.shape[0])\n",
    "    for i in range(oof.shape[0]):\n",
    "        a, b, c = y_true, oof_clip[i], 0\n",
    "        for j in range(oof.shape[0]):\n",
    "            if j != i:\n",
    "                c += weights[j] * oof_clip[j]\n",
    "        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n",
    "    return gradients\n",
    "\n",
    "oof = []\n",
    "y_true = []\n",
    "def find_optimal_blend(predictions, train_data, target_columns):\n",
    "    \n",
    "    global oof, y_true\n",
    "    y_true = train_data.sort_values(by='sig_id')[target_columns].values\n",
    "    oof = np.zeros((len(predictions), y_true.shape[0], y_true.shape[1]))\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        oof[i] = pred.sort_values(by='sig_id')[target_columns].values\n",
    "\n",
    "    tol = 1e-10\n",
    "    init_guess = [1 / oof.shape[0]] * oof.shape[0]\n",
    "    bnds = [(0, 1) for _ in range(oof.shape[0])]\n",
    "    cons = {'type': 'eq', \n",
    "            'fun': lambda x: np.sum(x) - 1, \n",
    "            'jac': lambda x: [1] * len(x)}\n",
    "\n",
    "    res_scipy = minimize(fun = func_numpy_metric, \n",
    "                         x0 = init_guess, \n",
    "                         method = 'SLSQP', \n",
    "                         jac = grad_func, \n",
    "                         bounds = bnds, \n",
    "                         constraints = cons, \n",
    "                         tol = tol)\n",
    "    \n",
    "    return res_scipy.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026441,
     "end_time": "2020-12-30T07:53:13.845524",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.819083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.909554Z",
     "iopub.status.busy": "2020-12-30T07:53:13.908816Z",
     "iopub.status.idle": "2020-12-30T07:53:13.912720Z",
     "shell.execute_reply": "2020-12-30T07:53:13.912179Z"
    },
    "papermill": {
     "duration": 0.038984,
     "end_time": "2020-12-30T07:53:13.912837",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.873853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model, data_loader, target_columns):\n",
    "    predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        x = batch['x'].to(DEVICE)\n",
    "        batch_predictions = model(x).sigmoid().detach().cpu().numpy()\n",
    "        sig_ids = np.array(batch['sig_id'])\n",
    "\n",
    "        df = pd.DataFrame(batch_predictions, columns=target_columns)\n",
    "        df['sig_id'] = sig_ids\n",
    "        predictions.append(df)\n",
    "\n",
    "    return pd.concat(predictions).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:13.983607Z",
     "iopub.status.busy": "2020-12-30T07:53:13.982561Z",
     "iopub.status.idle": "2020-12-30T07:53:13.986157Z",
     "shell.execute_reply": "2020-12-30T07:53:13.985556Z"
    },
    "papermill": {
     "duration": 0.046663,
     "end_time": "2020-12-30T07:53:13.986262",
     "exception": false,
     "start_time": "2020-12-30T07:53:13.939599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_log_loss(predicted_df, train_df, target_columns):\n",
    "    predicted_df = predicted_df.copy()\n",
    "    train_df = train_df.copy()\n",
    "    \n",
    "    predicted_df = predicted_df[target_columns + ['sig_id']].reset_index(drop=True)\n",
    "    predicted_df = predicted_df.sort_values(by=['sig_id'])\n",
    "    predicted_df = predicted_df.drop('sig_id', axis=1)\n",
    "\n",
    "    true_df = train_df[target_columns + ['sig_id']].reset_index(drop=True)\n",
    "    true_df = true_df.sort_values(by=['sig_id'])\n",
    "    true_df = true_df.drop('sig_id', axis=1)\n",
    "\n",
    "    predicted_values = predicted_df.values\n",
    "    true_values = true_df.values\n",
    "    \n",
    "    score = 0\n",
    "    loss_per_class = []\n",
    "    for i in range(predicted_values.shape[1]):\n",
    "        _score = log_loss(true_values[:, i].astype(np.float), predicted_values[:, i].astype(np.float), eps=1e-15, labels=[1,0])\n",
    "        loss_per_class.append(_score)\n",
    "        score += _score / predicted_values.shape[1]\n",
    "\n",
    "    return score, loss_per_class\n",
    "\n",
    "def scale_predictions(predictions, target_columns, scale_values=None):\n",
    "    predictions = [p.copy() for p in predictions]\n",
    "    predictions = [p.sort_values(by=['sig_id']).reset_index(drop=True) for p in predictions]\n",
    "    \n",
    "    final_predictions = np.zeros((predictions[0].shape[0], len(target_columns)))\n",
    "    \n",
    "    for i, p in enumerate(predictions):\n",
    "        p_values = p[target_columns].values\n",
    "        \n",
    "        if scale_values is None:\n",
    "            final_predictions += p_values / len(predictions)\n",
    "        else:\n",
    "            final_predictions += (p_values * scale_values[i])\n",
    "        \n",
    "    predictions_df = predictions[0].copy()\n",
    "    predictions_df.loc[:, target_columns] = final_predictions\n",
    "    \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:14.057602Z",
     "iopub.status.busy": "2020-12-30T07:53:14.056617Z",
     "iopub.status.idle": "2020-12-30T07:53:14.059883Z",
     "shell.execute_reply": "2020-12-30T07:53:14.060463Z"
    },
    "papermill": {
     "duration": 0.044932,
     "end_time": "2020-12-30T07:53:14.060614",
     "exception": false,
     "start_time": "2020-12-30T07:53:14.015682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainFactory:\n",
    "    \n",
    "    @classmethod\n",
    "    def meta_model(cls, train_loader, epochs):\n",
    "        global model_config, DEVICE\n",
    "        \n",
    "        model = MetaModel(model_config).to(DEVICE)\n",
    "        best_model = MetaModel(model_config).to(DEVICE)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                     lr=1e-3,\n",
    "                                     weight_decay=1e-5)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "                                                        max_lr=1e-2,\n",
    "                                                        epochs=epochs, \n",
    "                                                        steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        return model, best_model, optimizer, scheduler, loss_fn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:14.137514Z",
     "iopub.status.busy": "2020-12-30T07:53:14.136630Z",
     "iopub.status.idle": "2020-12-30T07:53:14.141053Z",
     "shell.execute_reply": "2020-12-30T07:53:14.140515Z"
    },
    "papermill": {
     "duration": 0.049652,
     "end_time": "2020-12-30T07:53:14.141191",
     "exception": false,
     "start_time": "2020-12-30T07:53:14.091539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, best_model, optimizer, scheduler, loss_fn, train_loader, valid_loader, epochs):\n",
    "    global gene_features, cell_features, target_columns\n",
    "    \n",
    "    train_data = train_loader.dataset.dataset_df\n",
    "    valid_data = valid_loader.dataset.dataset_df\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for train_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = train_batch['x'].to(DEVICE)\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            y_true = train_batch['y'].to(DEVICE)\n",
    "            \n",
    "            curr_train_loss = loss_fn(y_pred, y_true)\n",
    "            \n",
    "            curr_train_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += ( curr_train_loss.item() * (len(train_batch['sig_id']) / len(train_data)))\n",
    "            \n",
    "            \n",
    "        valid_predictions = inference(model, valid_loader, target_columns)\n",
    "        valid_loss, _ = calculate_log_loss(valid_predictions, valid_data, target_columns)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "                           \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch:{epoch} \\t train_loss:{train_loss:.10f} \\t valid_loss:{valid_loss:.10f}')\n",
    "            \n",
    "    \n",
    "    valid_predictions = inference(best_model, valid_loader, target_columns)\n",
    "    \n",
    "    return best_model, valid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:14.204064Z",
     "iopub.status.busy": "2020-12-30T07:53:14.203197Z",
     "iopub.status.idle": "2020-12-30T07:53:14.206291Z",
     "shell.execute_reply": "2020-12-30T07:53:14.206819Z"
    },
    "papermill": {
     "duration": 0.037495,
     "end_time": "2020-12-30T07:53:14.206973",
     "exception": false,
     "start_time": "2020-12-30T07:53:14.169478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "FOLDS = 5\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "SEEDS = [11, 221, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:14.277135Z",
     "iopub.status.busy": "2020-12-30T07:53:14.268826Z",
     "iopub.status.idle": "2020-12-30T07:53:15.327409Z",
     "shell.execute_reply": "2020-12-30T07:53:15.326808Z"
    },
    "papermill": {
     "duration": 1.091473,
     "end_time": "2020-12-30T07:53:15.327526",
     "exception": false,
     "start_time": "2020-12-30T07:53:14.236053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Creating the cross validation strategy\n",
    "train_data = create_cross_validation_strategy(train_data, target_columns, FOLDS, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:15.397889Z",
     "iopub.status.busy": "2020-12-30T07:53:15.396747Z",
     "iopub.status.idle": "2020-12-30T07:53:15.645055Z",
     "shell.execute_reply": "2020-12-30T07:53:15.644512Z"
    },
    "papermill": {
     "duration": 0.287827,
     "end_time": "2020-12-30T07:53:15.645181",
     "exception": false,
     "start_time": "2020-12-30T07:53:15.357354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data[train_data.cp_type == 'trt_cp'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:15.713144Z",
     "iopub.status.busy": "2020-12-30T07:53:15.711114Z",
     "iopub.status.idle": "2020-12-30T07:53:15.713846Z",
     "shell.execute_reply": "2020-12-30T07:53:15.714376Z"
    },
    "papermill": {
     "duration": 0.039002,
     "end_time": "2020-12-30T07:53:15.714506",
     "exception": false,
     "start_time": "2020-12-30T07:53:15.675504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelTrainConfig:\n",
    "    def __init__(self, model_name, factory_func, scaling_func):\n",
    "        self.model_name = model_name\n",
    "        self.factory_func = factory_func\n",
    "        self.scaling_func = scaling_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:15.777917Z",
     "iopub.status.busy": "2020-12-30T07:53:15.777187Z",
     "iopub.status.idle": "2020-12-30T07:53:15.780849Z",
     "shell.execute_reply": "2020-12-30T07:53:15.781669Z"
    },
    "papermill": {
     "duration": 0.038374,
     "end_time": "2020-12-30T07:53:15.781815",
     "exception": false,
     "start_time": "2020-12-30T07:53:15.743441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version1 = ModelTrainConfig(model_name='meta_model', \n",
    "                                  factory_func=TrainFactory.meta_model, \n",
    "                                  scaling_func=None)\n",
    "\n",
    "\n",
    "models_train_configs = [model_version1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T07:53:15.857575Z",
     "iopub.status.busy": "2020-12-30T07:53:15.856610Z",
     "iopub.status.idle": "2020-12-30T08:14:20.967088Z",
     "shell.execute_reply": "2020-12-30T08:14:20.966185Z"
    },
    "papermill": {
     "duration": 1265.155678,
     "end_time": "2020-12-30T08:14:20.967213",
     "exception": false,
     "start_time": "2020-12-30T07:53:15.811535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:meta_model\n",
      "Training fold: 0\n",
      "Epoch:4 \t train_loss:0.0172933366 \t valid_loss:0.0178684464\n",
      "Epoch:9 \t train_loss:0.0171932050 \t valid_loss:0.0177406156\n",
      "Epoch:14 \t train_loss:0.0169966765 \t valid_loss:0.0176684645\n",
      "Epoch:19 \t train_loss:0.0166763352 \t valid_loss:0.0176385989\n",
      "Epoch:24 \t train_loss:0.0160084534 \t valid_loss:0.0171542757\n",
      "Epoch:29 \t train_loss:0.0154196326 \t valid_loss:0.0170687272\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "Epoch:4 \t train_loss:0.0170413376 \t valid_loss:0.0185043888\n",
      "Epoch:9 \t train_loss:0.0170268316 \t valid_loss:0.0182676448\n",
      "Epoch:14 \t train_loss:0.0168399262 \t valid_loss:0.0183783528\n",
      "Epoch:19 \t train_loss:0.0165063952 \t valid_loss:0.0179963072\n",
      "Epoch:24 \t train_loss:0.0157790861 \t valid_loss:0.0180326938\n",
      "Epoch:29 \t train_loss:0.0152041844 \t valid_loss:0.0179787932\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "Epoch:4 \t train_loss:0.0171500949 \t valid_loss:0.0186164387\n",
      "Epoch:9 \t train_loss:0.0174500881 \t valid_loss:0.0201661350\n",
      "Epoch:14 \t train_loss:0.0175640494 \t valid_loss:0.0193759477\n",
      "Epoch:19 \t train_loss:0.0166202091 \t valid_loss:0.0181107042\n",
      "Epoch:24 \t train_loss:0.0161260341 \t valid_loss:0.0177289162\n",
      "Epoch:29 \t train_loss:0.0154327002 \t valid_loss:0.0176744426\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "Epoch:4 \t train_loss:0.0173201187 \t valid_loss:0.0178256875\n",
      "Epoch:9 \t train_loss:0.0175954580 \t valid_loss:0.0175607420\n",
      "Epoch:14 \t train_loss:0.0169956766 \t valid_loss:0.0179155349\n",
      "Epoch:19 \t train_loss:0.0167191292 \t valid_loss:0.0173670104\n",
      "Epoch:24 \t train_loss:0.0159971370 \t valid_loss:0.0171672988\n",
      "Epoch:29 \t train_loss:0.0154112470 \t valid_loss:0.0172162674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "Epoch:4 \t train_loss:0.0172569811 \t valid_loss:0.0200622513\n",
      "Epoch:9 \t train_loss:0.0171162586 \t valid_loss:0.0181649992\n",
      "Epoch:14 \t train_loss:0.0173929444 \t valid_loss:0.0184262338\n",
      "Epoch:19 \t train_loss:0.0168644523 \t valid_loss:0.0176428807\n",
      "Epoch:24 \t train_loss:0.0161392835 \t valid_loss:0.0173932017\n",
      "Epoch:29 \t train_loss:0.0155253771 \t valid_loss:0.0172702105\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:meta_model \t Seed:11 \t oof_loss:0.0173612655\n",
      "Training fold: 0\n",
      "Epoch:4 \t train_loss:0.0173026893 \t valid_loss:0.0179718378\n",
      "Epoch:9 \t train_loss:0.0172228166 \t valid_loss:0.0179489559\n",
      "Epoch:14 \t train_loss:0.0170227270 \t valid_loss:0.0178623899\n",
      "Epoch:19 \t train_loss:0.0166531197 \t valid_loss:0.0173310902\n",
      "Epoch:24 \t train_loss:0.0159935649 \t valid_loss:0.0172717545\n",
      "Epoch:29 \t train_loss:0.0154179620 \t valid_loss:0.0169925556\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "Epoch:4 \t train_loss:0.0171239976 \t valid_loss:0.0185326855\n",
      "Epoch:9 \t train_loss:0.0170418066 \t valid_loss:0.0183513431\n",
      "Epoch:14 \t train_loss:0.0168733864 \t valid_loss:0.0181836121\n",
      "Epoch:19 \t train_loss:0.0165516360 \t valid_loss:0.0184872367\n",
      "Epoch:24 \t train_loss:0.0157839978 \t valid_loss:0.0180881303\n",
      "Epoch:29 \t train_loss:0.0152093337 \t valid_loss:0.0179270640\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "Epoch:4 \t train_loss:0.0173200721 \t valid_loss:0.0195777846\n",
      "Epoch:9 \t train_loss:0.0175107161 \t valid_loss:0.0185283269\n",
      "Epoch:14 \t train_loss:0.0171244212 \t valid_loss:0.0182771055\n",
      "Epoch:19 \t train_loss:0.0168527281 \t valid_loss:0.0178637724\n",
      "Epoch:24 \t train_loss:0.0160148996 \t valid_loss:0.0175768236\n",
      "Epoch:29 \t train_loss:0.0154932779 \t valid_loss:0.0176245877\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "Epoch:4 \t train_loss:0.0172750570 \t valid_loss:0.0182665913\n",
      "Epoch:9 \t train_loss:0.0175103479 \t valid_loss:0.0178884436\n",
      "Epoch:14 \t train_loss:0.0170330903 \t valid_loss:0.0181296225\n",
      "Epoch:19 \t train_loss:0.0166152307 \t valid_loss:0.0177069303\n",
      "Epoch:24 \t train_loss:0.0159694674 \t valid_loss:0.0173340607\n",
      "Epoch:29 \t train_loss:0.0153855825 \t valid_loss:0.0172320111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "Epoch:4 \t train_loss:0.0172979080 \t valid_loss:0.0183892802\n",
      "Epoch:9 \t train_loss:0.0172891008 \t valid_loss:0.0198030926\n",
      "Epoch:14 \t train_loss:0.0171118496 \t valid_loss:0.0187074625\n",
      "Epoch:19 \t train_loss:0.0166922195 \t valid_loss:0.0176493288\n",
      "Epoch:24 \t train_loss:0.0160787622 \t valid_loss:0.0173367584\n",
      "Epoch:29 \t train_loss:0.0154971402 \t valid_loss:0.0172880954\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:meta_model \t Seed:221 \t oof_loss:0.0173751934\n",
      "Training fold: 0\n",
      "Epoch:4 \t train_loss:0.0172445193 \t valid_loss:0.0178317040\n",
      "Epoch:9 \t train_loss:0.0171342609 \t valid_loss:0.0177472239\n",
      "Epoch:14 \t train_loss:0.0170295782 \t valid_loss:0.0177831477\n",
      "Epoch:19 \t train_loss:0.0166242717 \t valid_loss:0.0174343973\n",
      "Epoch:24 \t train_loss:0.0159842625 \t valid_loss:0.0172778452\n",
      "Epoch:29 \t train_loss:0.0153931787 \t valid_loss:0.0170848940\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "Epoch:4 \t train_loss:0.0170586611 \t valid_loss:0.0186035808\n",
      "Epoch:9 \t train_loss:0.0170798256 \t valid_loss:0.0184472377\n",
      "Epoch:14 \t train_loss:0.0168555326 \t valid_loss:0.0181204255\n",
      "Epoch:19 \t train_loss:0.0164631684 \t valid_loss:0.0180456435\n",
      "Epoch:24 \t train_loss:0.0158534469 \t valid_loss:0.0179234782\n",
      "Epoch:29 \t train_loss:0.0152158028 \t valid_loss:0.0178983071\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "Epoch:4 \t train_loss:0.0176330668 \t valid_loss:0.0201381155\n",
      "Epoch:9 \t train_loss:0.0170774969 \t valid_loss:0.0210650816\n",
      "Epoch:14 \t train_loss:0.0171229090 \t valid_loss:0.0180668725\n",
      "Epoch:19 \t train_loss:0.0167077449 \t valid_loss:0.0180032756\n",
      "Epoch:24 \t train_loss:0.0159127043 \t valid_loss:0.0175710608\n",
      "Epoch:29 \t train_loss:0.0153863772 \t valid_loss:0.0176192578\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "Epoch:4 \t train_loss:0.0172913926 \t valid_loss:0.0180414014\n",
      "Epoch:9 \t train_loss:0.0172279158 \t valid_loss:0.0181579181\n",
      "Epoch:14 \t train_loss:0.0170499249 \t valid_loss:0.0178768746\n",
      "Epoch:19 \t train_loss:0.0165890002 \t valid_loss:0.0176739774\n",
      "Epoch:24 \t train_loss:0.0159004210 \t valid_loss:0.0174435556\n",
      "Epoch:29 \t train_loss:0.0153477577 \t valid_loss:0.0172936568\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "Epoch:4 \t train_loss:0.0174456986 \t valid_loss:0.0187799798\n",
      "Epoch:9 \t train_loss:0.0177480301 \t valid_loss:0.0198440242\n",
      "Epoch:14 \t train_loss:0.0171908448 \t valid_loss:0.0193803249\n",
      "Epoch:19 \t train_loss:0.0167951016 \t valid_loss:0.0175744263\n",
      "Epoch:24 \t train_loss:0.0160367391 \t valid_loss:0.0173651553\n",
      "Epoch:29 \t train_loss:0.0154721736 \t valid_loss:0.0172263326\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:meta_model \t Seed:50 \t oof_loss:0.0174045952\n",
      "Model:meta_model \t valid_loss:0.0172494828\n"
     ]
    }
   ],
   "source": [
    "models_valid_predictions = []\n",
    "models_test_predictions = []\n",
    "\n",
    "seed_losses = []\n",
    "\n",
    "for model_train_config in models_train_configs:\n",
    "    print(f'Training model:{model_train_config.model_name}')\n",
    "    \n",
    "    single_model_valid_predictions = []\n",
    "    single_model_test_predictions = []\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        seed_everything(seed)\n",
    "\n",
    "        model_seed_valid_predictions = []\n",
    "        model_seed_test_predictions = []\n",
    "\n",
    "        for fold in range(FOLDS):\n",
    "            print(f'Training fold: {fold}')\n",
    "\n",
    "            train_loader, valid_loader = preprocess_fold_data(train_data=train_data, \n",
    "                                                              fold=fold, \n",
    "                                                              scaling_func=model_train_config.scaling_func)\n",
    "            \n",
    "            \n",
    "\n",
    "            model_config = ModelConfig(number_of_features=len(feature_columns),\n",
    "                                       number_of_targets=len(target_columns))\n",
    "\n",
    "            model, best_model, optimizer, scheduler, loss_fn = model_train_config.factory_func(train_loader, EPOCHS)\n",
    "\n",
    "            best_model, valid_predictions = train_model(model=model,\n",
    "                                                        best_model=best_model,\n",
    "                                                        optimizer=optimizer, \n",
    "                                                        scheduler=scheduler, \n",
    "                                                        loss_fn=loss_fn, \n",
    "                                                        train_loader=train_loader, \n",
    "                                                        valid_loader=valid_loader,\n",
    "                                                        epochs=EPOCHS)\n",
    "\n",
    "            #TODO: Save the model here.\n",
    "            torch.save(best_model.state_dict(), f'model-{model_train_config.model_name}_fold-{fold}_seed-{seed}')\n",
    "            \n",
    "            model_seed_valid_predictions.append(valid_predictions)\n",
    "        \n",
    "            print('-' * 100)\n",
    "\n",
    "\n",
    "        valid_predictions = pd.concat(model_seed_valid_predictions).reset_index(drop=True)\n",
    "        \n",
    "        single_model_valid_predictions.append(valid_predictions)\n",
    "        \n",
    "        valid_loss, _ = calculate_log_loss(valid_predictions, train_data, target_columns)\n",
    "\n",
    "        seed_losses.append(valid_loss)\n",
    "\n",
    "        print(f'Model:{model_train_config.model_name} \\t Seed:{seed} \\t oof_loss:{valid_loss:.10f}')\n",
    "\n",
    "    valid_predictions = scale_predictions(single_model_valid_predictions, target_columns)\n",
    "    \n",
    "    models_valid_predictions.append(valid_predictions)\n",
    "    \n",
    "    \n",
    "    valid_loss, _ = calculate_log_loss(valid_predictions, train_data, target_columns)\n",
    "\n",
    "    print(f'Model:{model_train_config.model_name} \\t valid_loss:{valid_loss:.10f}')\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T08:14:21.104117Z",
     "iopub.status.busy": "2020-12-30T08:14:21.103157Z",
     "iopub.status.idle": "2020-12-30T08:14:21.784788Z",
     "shell.execute_reply": "2020-12-30T08:14:21.784229Z"
    },
    "papermill": {
     "duration": 0.753458,
     "end_time": "2020-12-30T08:14:21.784917",
     "exception": false,
     "start_time": "2020-12-30T08:14:21.031459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal blend weights: [1.]\n"
     ]
    }
   ],
   "source": [
    "#Finding optimal blend weights\n",
    "blend_weights = find_optimal_blend(models_valid_predictions, train_data, target_columns)\n",
    "\n",
    "print(f'Optimal blend weights: {blend_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T08:14:21.921518Z",
     "iopub.status.busy": "2020-12-30T08:14:21.920249Z",
     "iopub.status.idle": "2020-12-30T08:14:23.645986Z",
     "shell.execute_reply": "2020-12-30T08:14:23.645117Z"
    },
    "papermill": {
     "duration": 1.795782,
     "end_time": "2020-12-30T08:14:23.646107",
     "exception": false,
     "start_time": "2020-12-30T08:14:21.850325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_predictions = scale_predictions(models_valid_predictions, target_columns, blend_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T08:14:23.781479Z",
     "iopub.status.busy": "2020-12-30T08:14:23.780273Z",
     "iopub.status.idle": "2020-12-30T08:14:24.802648Z",
     "shell.execute_reply": "2020-12-30T08:14:24.803194Z"
    },
    "papermill": {
     "duration": 1.092041,
     "end_time": "2020-12-30T08:14:24.803333",
     "exception": false,
     "start_time": "2020-12-30T08:14:23.711292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.017249482755610626\n"
     ]
    }
   ],
   "source": [
    "validation_loss, _ = calculate_log_loss(valid_predictions, train_data, target_columns)\n",
    "print(f'Validation loss: {validation_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T08:14:24.940552Z",
     "iopub.status.busy": "2020-12-30T08:14:24.938797Z",
     "iopub.status.idle": "2020-12-30T08:14:24.943174Z",
     "shell.execute_reply": "2020-12-30T08:14:24.942607Z"
    },
    "papermill": {
     "duration": 0.074326,
     "end_time": "2020-12-30T08:14:24.943272",
     "exception": false,
     "start_time": "2020-12-30T08:14:24.868946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed loss std: 0.0000180614\n"
     ]
    }
   ],
   "source": [
    "print(f'Seed loss std: {np.array(seed_losses).std():.10f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1306.23049,
   "end_time": "2020-12-30T08:14:26.422929",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-30T07:52:40.192439",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
