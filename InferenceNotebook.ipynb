{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-30T11:56:43.652640Z",
     "iopub.status.busy": "2020-12-30T11:56:43.651933Z",
     "iopub.status.idle": "2020-12-30T11:56:56.529912Z",
     "shell.execute_reply": "2020-12-30T11:56:56.528790Z"
    },
    "papermill": {
     "duration": 12.919855,
     "end_time": "2020-12-30T11:56:56.530049",
     "exception": false,
     "start_time": "2020-12-30T11:56:43.610194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import datetime\n",
    "from scipy.optimize import minimize, fsolve\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import QuantileTransformer, RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "\n",
    "import copy\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-30T11:56:56.595634Z",
     "iopub.status.busy": "2020-12-30T11:56:56.594951Z",
     "iopub.status.idle": "2020-12-30T11:56:56.601637Z",
     "shell.execute_reply": "2020-12-30T11:56:56.601030Z"
    },
    "papermill": {
     "duration": 0.041878,
     "end_time": "2020-12-30T11:56:56.601739",
     "exception": false,
     "start_time": "2020-12-30T11:56:56.559861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED=42\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029173,
     "end_time": "2020-12-30T11:56:56.660482",
     "exception": false,
     "start_time": "2020-12-30T11:56:56.631309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:56:56.726748Z",
     "iopub.status.busy": "2020-12-30T11:56:56.726149Z",
     "iopub.status.idle": "2020-12-30T11:57:03.981901Z",
     "shell.execute_reply": "2020-12-30T11:57:03.981081Z"
    },
    "papermill": {
     "duration": 7.291266,
     "end_time": "2020-12-30T11:57:03.982069",
     "exception": false,
     "start_time": "2020-12-30T11:56:56.690803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "test_data = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "train_drug_ids = pd.read_csv('../input/lish-moa/train_drug.csv') \n",
    "\n",
    "train_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "\n",
    "train_data = train_features.merge(train_targets, on='sig_id', how='left')\n",
    "train_data = train_data.merge(train_drug_ids, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.051402Z",
     "iopub.status.busy": "2020-12-30T11:57:04.049635Z",
     "iopub.status.idle": "2020-12-30T11:57:04.052046Z",
     "shell.execute_reply": "2020-12-30T11:57:04.052515Z"
    },
    "papermill": {
     "duration": 0.039117,
     "end_time": "2020-12-30T11:57:04.052638",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.013521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_columns = [c for c in train_targets.columns if c != 'sig_id']\n",
    "gene_features = [col for col in train_features.columns if col.startswith('g-')]\n",
    "cell_features = [col for col in train_features.columns if col.startswith('c-')]\n",
    "feature_columns = gene_features + cell_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030065,
     "end_time": "2020-12-30T11:57:04.148183",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.118118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Cross validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.226248Z",
     "iopub.status.busy": "2020-12-30T11:57:04.224267Z",
     "iopub.status.idle": "2020-12-30T11:57:04.226949Z",
     "shell.execute_reply": "2020-12-30T11:57:04.227439Z"
    },
    "papermill": {
     "duration": 0.049018,
     "end_time": "2020-12-30T11:57:04.227564",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.178546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_cross_validation_strategy(data, targets, FOLDS, SEED):\n",
    "\n",
    "    vc = data.drug_id.value_counts()\n",
    "    \n",
    "#     vc1 = vc.loc[(vc==6)|(vc==12)|(vc==18)].index.sort_values()\n",
    "#     vc2 = vc.loc[(vc!=6)&(vc!=12)&(vc!=18)].index.sort_values()\n",
    "    \n",
    "    vc1 = vc.loc[vc <= 19].index.sort_values()\n",
    "    vc2 = vc.loc[vc > 19].index.sort_values()\n",
    "\n",
    "    dct1 = {} \n",
    "    dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    tmp = data.groupby('drug_id')[targets].mean().loc[vc1]\n",
    "    \n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp.index[idxV].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "    tmp = data.loc[data.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "    \n",
    "    for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets])):\n",
    "        dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    # ASSIGN FOLDS\n",
    "    data['fold'] = data.drug_id.map(dct1)\n",
    "    data.loc[data.fold.isna(),'fold'] = data.loc[data.fold.isna(),'sig_id'].map(dct2)\n",
    "    data.fold = data.fold.astype('int8')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029796,
     "end_time": "2020-12-30T11:57:04.287973",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.258177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.368906Z",
     "iopub.status.busy": "2020-12-30T11:57:04.368143Z",
     "iopub.status.idle": "2020-12-30T11:57:04.371201Z",
     "shell.execute_reply": "2020-12-30T11:57:04.370742Z"
    },
    "papermill": {
     "duration": 0.053331,
     "end_time": "2020-12-30T11:57:04.371298",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.317967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoaDataset:\n",
    "    def __init__(self, dataset_df, gene_features, cell_features, target_ids):\n",
    "        self.dataset_df = dataset_df\n",
    "        self.target_ids = target_ids\n",
    "    \n",
    "        self.gene_features = self.dataset_df[gene_features].values\n",
    "        self.cell_features = self.dataset_df[cell_features].values\n",
    "        self.targets = None\n",
    "    \n",
    "        if self.target_ids is not None:\n",
    "            self.targets = self.dataset_df[target_ids].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "    \n",
    "    def number_of_features(self):\n",
    "        return self.gene_features.shape[1], self.cell_features.shape[1]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        dataset_sample = {}\n",
    "\n",
    "        dataset_sample['genes'] = torch.tensor(self.gene_features[item, :], dtype=torch.float)\n",
    "        dataset_sample['cells'] = torch.tensor(self.cell_features[item, :], dtype=torch.float)\n",
    "        dataset_sample['sig_id'] = self.dataset_df.loc[item, 'sig_id']\n",
    "\n",
    "        if self.target_ids is not None:\n",
    "            dataset_sample['y'] = torch.tensor(self.targets[item, :], dtype=torch.float)\n",
    "\n",
    "        return dataset_sample\n",
    "    \n",
    "    \n",
    "class MoaMetaDataset:\n",
    "    def __init__(self, dataset_df, feature_ids, target_ids):\n",
    "        self.dataset_df = dataset_df\n",
    "        self.feature_ids = feature_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.num_models = len(feature_ids) // 206\n",
    "\n",
    "        # samples x models x targets\n",
    "        self.features = self.dataset_df[feature_ids].values\n",
    "        self.targets = None\n",
    "\n",
    "        if self.target_ids is not None:\n",
    "            self.targets = self.dataset_df[target_ids].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "\n",
    "    def num_of_features(self):\n",
    "        return len(feature_ids)\n",
    "\n",
    "    def num_of_targets(self):\n",
    "        return None if self.target_ids is None else len(self.target_ids)\n",
    "\n",
    "    def get_ids(self):\n",
    "        return self.dataset_df.sig_id.values\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return_item = {}\n",
    "        \n",
    "        return_item['x'] = torch.tensor(self.features[item, :].reshape(self.num_models, 206), dtype=torch.float)\n",
    "        return_item['sig_id'] = self.dataset_df.loc[item, 'sig_id']\n",
    "\n",
    "        if self.target_ids is not None:\n",
    "            return_item['y'] = torch.tensor(self.targets[item, :], dtype=torch.float)\n",
    "\n",
    "        return return_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.438671Z",
     "iopub.status.busy": "2020-12-30T11:57:04.436916Z",
     "iopub.status.idle": "2020-12-30T11:57:04.439299Z",
     "shell.execute_reply": "2020-12-30T11:57:04.439788Z"
    },
    "papermill": {
     "duration": 0.03852,
     "end_time": "2020-12-30T11:57:04.439897",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.401377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, number_of_features, number_of_genes, number_of_cells, number_of_targets):\n",
    "        self.number_of_features = number_of_features\n",
    "        self.number_of_genes = number_of_genes\n",
    "        self.number_of_cells = number_of_cells\n",
    "        self.number_of_targets = number_of_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.517882Z",
     "iopub.status.busy": "2020-12-30T11:57:04.516157Z",
     "iopub.status.idle": "2020-12-30T11:57:04.518954Z",
     "shell.execute_reply": "2020-12-30T11:57:04.519465Z"
    },
    "papermill": {
     "duration": 0.049142,
     "end_time": "2020-12-30T11:57:04.519622",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.470480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoaModelBlock(nn.Module):\n",
    "    def __init__(self, num_in, num_out, dropout, weight_norm=False, ):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(num_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if weight_norm:\n",
    "            self.linear = nn.utils.weight_norm(nn.Linear(num_in, num_out))\n",
    "        else:\n",
    "            self.linear = nn.Linear(num_in, num_out)\n",
    "        \n",
    "        self.activation = nn.PReLU(num_out)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class MoaEncodeBlock(nn.Module):\n",
    "    def __init__(self, num_in, num_out, dropout, weight_norm=False):\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(num_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if weight_norm:\n",
    "            self.linear = nn.utils.weight_norm(nn.Linear(num_in, num_out))\n",
    "        else:\n",
    "            self.linear = nn.Linear(num_in, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.591065Z",
     "iopub.status.busy": "2020-12-30T11:57:04.590381Z",
     "iopub.status.idle": "2020-12-30T11:57:04.593437Z",
     "shell.execute_reply": "2020-12-30T11:57:04.593872Z"
    },
    "papermill": {
     "duration": 0.043139,
     "end_time": "2020-12-30T11:57:04.593988",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.550849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoaModel_V1(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        total_features = model_config.number_of_genes + model_config.number_of_cells\n",
    "        dropout = 0.15\n",
    "        hidden_size = 1024\n",
    "        \n",
    "        self.block1 = MoaModelBlock(total_features, 2048, dropout)\n",
    "        self.block2 = MoaModelBlock(2048, 1024, dropout)\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.BatchNorm1d(1024),\n",
    "                          nn.Dropout(dropout),\n",
    "                          nn.Linear(1024, model_config.number_of_targets))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_genes = data['genes']\n",
    "        x_cells = data['cells']\n",
    "        \n",
    "        x = torch.cat((x_genes, x_cells), dim=1)\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.668508Z",
     "iopub.status.busy": "2020-12-30T11:57:04.667670Z",
     "iopub.status.idle": "2020-12-30T11:57:04.671092Z",
     "shell.execute_reply": "2020-12-30T11:57:04.670595Z"
    },
    "papermill": {
     "duration": 0.046588,
     "end_time": "2020-12-30T11:57:04.671181",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.624593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoaModel_V2(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        dropout = 0.15\n",
    "        hidden_size = 512\n",
    "        \n",
    "        self.genes_encoder = MoaEncodeBlock(model_config.number_of_genes, 128, dropout)\n",
    "            \n",
    "        self.cells_encoder = MoaEncodeBlock(model_config.number_of_cells, 32, dropout)\n",
    "        \n",
    "        out_encodings = 128 + 32\n",
    "    \n",
    "        self.block1 = MoaModelBlock(128, hidden_size, dropout)\n",
    "        self.block2 = MoaModelBlock(32, hidden_size, dropout)\n",
    "        \n",
    "        self.block3 = MoaModelBlock(hidden_size, hidden_size, dropout)\n",
    "        self.block4 = MoaModelBlock(hidden_size, hidden_size, dropout)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.BatchNorm1d(hidden_size),\n",
    "                          nn.Dropout(dropout),\n",
    "                          nn.Linear(hidden_size, model_config.number_of_targets))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_genes = data['genes'].to(DEVICE)\n",
    "        x_cells = data['cells'].to(DEVICE)\n",
    "        \n",
    "        encoded_genes = self.genes_encoder(x_genes)\n",
    "        encoded_cells = self.cells_encoder(x_cells)\n",
    "        \n",
    "        x_genes = self.block1(encoded_genes)\n",
    "        x_cells = self.block2(encoded_cells)\n",
    "\n",
    "        x = self.block3(x_genes + x_cells)\n",
    "        x = self.block4(x)\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.746443Z",
     "iopub.status.busy": "2020-12-30T11:57:04.744736Z",
     "iopub.status.idle": "2020-12-30T11:57:04.747091Z",
     "shell.execute_reply": "2020-12-30T11:57:04.747569Z"
    },
    "papermill": {
     "duration": 0.04569,
     "end_time": "2020-12-30T11:57:04.747674",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.701984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoaModel_V3(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        dropout = 0.15\n",
    "        hidden_size = 512\n",
    "        \n",
    "        self.genes_encoder = MoaEncodeBlock(model_config.number_of_genes, 128, dropout)\n",
    "            \n",
    "        self.cells_encoder = MoaEncodeBlock(model_config.number_of_cells, 32, dropout)\n",
    "            \n",
    "        out_encodings = 128 + 32\n",
    "    \n",
    "        self.block1 = MoaModelBlock(out_encodings, hidden_size, dropout)\n",
    "        self.block2 = MoaModelBlock(hidden_size, hidden_size, dropout)\n",
    "        self.model = nn.Sequential(\n",
    "                          nn.BatchNorm1d(hidden_size),\n",
    "                          nn.Dropout(dropout),\n",
    "                          nn.Linear(hidden_size, model_config.number_of_targets))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_genes = data['genes'].to(DEVICE)\n",
    "        x_cells = data['cells'].to(DEVICE)\n",
    "        \n",
    "        encoded_genes = self.genes_encoder(x_genes)\n",
    "        encoded_cells = self.cells_encoder(x_cells)\n",
    "               \n",
    "        x = torch.cat((encoded_genes, encoded_cells), 1)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.835188Z",
     "iopub.status.busy": "2020-12-30T11:57:04.827500Z",
     "iopub.status.idle": "2020-12-30T11:57:04.837877Z",
     "shell.execute_reply": "2020-12-30T11:57:04.837381Z"
    },
    "papermill": {
     "duration": 0.059301,
     "end_time": "2020-12-30T11:57:04.837965",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.778664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvFeatureExtractions(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, channel_1=256, channel_2=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channel_1 = channel_1\n",
    "        self.channel_2 = channel_2\n",
    "        self.final_conv_features = int(hidden_size / channel_1) * channel_2\n",
    "            \n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(0.15)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "               \n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(channel_1)\n",
    "        self.dropout_c1 = nn.Dropout(0.15)\n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(channel_1,channel_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
    "\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(channel_2)\n",
    "        self.dropout_c2 = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(channel_2,channel_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
    "        \n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "        self.final_conv_features = int(self.final_conv_features / 2)\n",
    "        \n",
    "        self.flt = nn.Flatten()\n",
    "     \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.celu(self.dense1(x), alpha=0.06)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.channel_1, -1)\n",
    "        \n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = self.dropout_c1(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        x = self.max_po_c2(x)\n",
    "        \n",
    "        x = self.flt(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "class MoaModel_V4(nn.Module):\n",
    "        def __init__(self, model_config):\n",
    "            super(MoaModel_V4, self).__init__()\n",
    "            hidden_size = 512\n",
    "            dropout = 0.15\n",
    "            \n",
    "            self.gene_cnn_features = ConvFeatureExtractions(model_config.number_of_genes, 2048, channel_1=128, channel_2=256)\n",
    "            self.cell_cnn_features = ConvFeatureExtractions(model_config.number_of_cells, 1024, channel_1=64, channel_2=128)\n",
    "            \n",
    "            encoded_features = self.gene_cnn_features.final_conv_features + self.cell_cnn_features.final_conv_features\n",
    "            \n",
    "            self.block1 = MoaModelBlock(encoded_features, hidden_size, dropout, weight_norm=True)\n",
    "            self.model = MoaEncodeBlock(hidden_size, model_config.number_of_targets, dropout, weight_norm=True)\n",
    "            \n",
    "\n",
    "        def forward(self, data):\n",
    "            \n",
    "            x_genes = data['genes'].to(DEVICE)\n",
    "            x_cells = data['cells'].to(DEVICE)\n",
    "\n",
    "            x_genes = self.gene_cnn_features(x_genes)\n",
    "            x_cells = self.cell_cnn_features(x_cells)\n",
    "            \n",
    "            x = torch.cat((x_genes, x_cells), dim=1)\n",
    "\n",
    "            x = self.block1(x)\n",
    "            x = self.model(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:04.910978Z",
     "iopub.status.busy": "2020-12-30T11:57:04.907148Z",
     "iopub.status.idle": "2020-12-30T11:57:04.913784Z",
     "shell.execute_reply": "2020-12-30T11:57:04.913239Z"
    },
    "papermill": {
     "duration": 0.044893,
     "end_time": "2020-12-30T11:57:04.913883",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.868990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        self.num_models = model_config.number_of_features // model_config.number_of_targets\n",
    "        self.model_config = model_config\n",
    "        \n",
    "        dropout = 0.15\n",
    "        hidden_size = 512\n",
    "        \n",
    "        self.encoders = nn.ModuleList([MoaEncodeBlock(model_config.number_of_targets, 64, dropout) for i in range(self.num_models)])\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(64, hidden_size),\n",
    "                                   nn.Dropout(dropout),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_size, hidden_size),\n",
    "                                   nn.Dropout(dropout),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_size, model_config.number_of_targets))\n",
    "        \n",
    "\n",
    "    def forward(self, data): # batch size x models x features\n",
    "        x = data['x'].to(DEVICE)\n",
    "        x_ = self.encoders[0](x[:, 0, :])\n",
    "        for i in range(1, self.num_models):\n",
    "            x_ = x_ + self.encoders[i](x[:, i, :]) \n",
    "        return self.model(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031567,
     "end_time": "2020-12-30T11:57:04.977670",
     "exception": false,
     "start_time": "2020-12-30T11:57:04.946103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Smooth loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:05.064465Z",
     "iopub.status.busy": "2020-12-30T11:57:05.062797Z",
     "iopub.status.idle": "2020-12-30T11:57:05.065371Z",
     "shell.execute_reply": "2020-12-30T11:57:05.065883Z"
    },
    "papermill": {
     "duration": 0.057184,
     "end_time": "2020-12-30T11:57:05.065994",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.008810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SmoothCrossEntropyLoss(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets, n_classes, smoothing=0.0):\n",
    "        assert 0 <= smoothing <= 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1 - smoothing) + torch.ones_like(targets).to(DEVICE) * smoothing / n_classes\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothCrossEntropyLoss()._smooth(targets, inputs.shape[1], self.smoothing)\n",
    "\n",
    "        if self.weight is not None:\n",
    "            inputs = inputs * self.weight.unsqueeze(0)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "        def __init__(self, weight=None, reduction='mean', smoothing=0.0,pos_weight = None):\n",
    "            super().__init__(weight=weight, reduction=reduction)\n",
    "            self.smoothing = smoothing\n",
    "            self.weight = weight\n",
    "            self.reduction = reduction\n",
    "            self.pos_weight = pos_weight\n",
    "\n",
    "        @staticmethod\n",
    "        def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "            assert 0 <= smoothing < 1\n",
    "            with torch.no_grad():\n",
    "                targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            return targets\n",
    "\n",
    "        def forward(self, inputs, targets):\n",
    "            targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "                self.smoothing)\n",
    "            loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
    "                                                      pos_weight = self.pos_weight)\n",
    "\n",
    "            if  self.reduction == 'sum':\n",
    "                loss = loss.sum()\n",
    "            elif  self.reduction == 'mean':\n",
    "                loss = loss.mean()\n",
    "\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031603,
     "end_time": "2020-12-30T11:57:05.130235",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.098632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scaling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:05.226932Z",
     "iopub.status.busy": "2020-12-30T11:57:05.225083Z",
     "iopub.status.idle": "2020-12-30T11:57:05.227649Z",
     "shell.execute_reply": "2020-12-30T11:57:05.228131Z"
    },
    "papermill": {
     "duration": 0.065792,
     "end_time": "2020-12-30T11:57:05.228244",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.162452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def true_rank_gaus_scale(data, columns): \n",
    "    global DEVICE\n",
    "    \n",
    "    if DEVICE == 'cuda':\n",
    "        import cupy as cp\n",
    "        from cupyx.scipy.special import erfinv\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        for f in columns:\n",
    "            r_gpu = cp.array(data[f].values)\n",
    "            r_gpu = r_gpu.argsort().argsort()\n",
    "            r_gpu = (r_gpu/r_gpu.max()-0.5)*2 \n",
    "            r_gpu = cp.clip(r_gpu,-1+epsilon,1-epsilon)\n",
    "            r_gpu = erfinv(r_gpu) \n",
    "            data[f] = cp.asnumpy( r_gpu * np.sqrt(2) )\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    from scipy.special import erfinv as sp_erfinv\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    for f in columns:\n",
    "        r_cpu = data[f].values.argsort().argsort()\n",
    "        r_cpu = (r_cpu/r_cpu.max()-0.5)*2 \n",
    "        r_cpu = np.clip(r_cpu,-1+epsilon,1-epsilon)\n",
    "        r_cpu = sp_erfinv(r_cpu) \n",
    "        data[f] = r_cpu * np.sqrt(2)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def quantile_dosetime_scaling(train_data, valid_data, test_data, feature_columns):\n",
    "    global RANDOM_SEED\n",
    "    \n",
    "    train_arr = []\n",
    "    valid_arr = []\n",
    "    test_arr = []\n",
    "    \n",
    "    for cp_dose in ['D1', 'D2']:\n",
    "        for cp_time in [24, 48, 72]:\n",
    "            temp_train = train_data[train_data.cp_dose == cp_dose].reset_index(drop=True)\n",
    "            temp_train = temp_train[temp_train.cp_time == cp_time].reset_index(drop=True)\n",
    "            \n",
    "            temp_valid = valid_data[valid_data.cp_dose == cp_dose].reset_index(drop=True)\n",
    "            temp_valid = temp_valid[temp_valid.cp_time == cp_time].reset_index(drop=True)\n",
    "            \n",
    "            temp_test = test_data[test_data.cp_dose == cp_dose].reset_index(drop=True)\n",
    "            temp_test = temp_test[temp_test.cp_time == cp_time].reset_index(drop=True)\n",
    "\n",
    "            scaler = QuantileTransformer(n_quantiles=100,random_state=RANDOM_SEED, output_distribution=\"normal\")\n",
    "            temp_train[feature_columns] = scaler.fit_transform(temp_train[feature_columns])\n",
    "            temp_valid[feature_columns] = scaler.transform(temp_valid[feature_columns])\n",
    "            temp_test[feature_columns] = scaler.transform(temp_test[feature_columns])\n",
    "            \n",
    "            train_arr.append(temp_train)\n",
    "            valid_arr.append(temp_valid)\n",
    "            test_arr.append(temp_test)\n",
    "            \n",
    "    train_data = pd.concat(train_arr).reset_index(drop=True)\n",
    "    valid_data = pd.concat(valid_arr).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_arr).reset_index(drop=True)\n",
    "            \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "def true_rankgaus_dosetime(data, columns):\n",
    "    global RANDOM_SEED\n",
    "    arr = []\n",
    "    \n",
    "    for cp_dose in ['D1', 'D2']:\n",
    "        for cp_time in [24, 48, 72]:\n",
    "            temp_data = data[data.cp_dose == cp_dose].reset_index(drop=True)\n",
    "            temp_data = temp_data[temp_data.cp_time == cp_time].reset_index(drop=True)\n",
    "            \n",
    "            arr.append(true_rank_gaus_scale(temp_data, columns))\n",
    "        \n",
    "    return pd.concat(arr).reset_index(drop=True)\n",
    "\n",
    "def true_rankgaus_dosetime_scaling(train_data, valid_data, test_data, feature_columns):\n",
    "    \n",
    "    train_data = true_rankgaus_dosetime(train_data, feature_columns)\n",
    "    valid_data = true_rankgaus_dosetime(valid_data, feature_columns)\n",
    "    test_data = true_rankgaus_dosetime(test_data, feature_columns)\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "    \n",
    "\n",
    "def true_rankgaus_scaling(train_data, valid_data, test_data, feature_columns):\n",
    "    \n",
    "    train_data = true_rank_gaus_scale(train_data, feature_columns)\n",
    "    valid_data = true_rank_gaus_scale(valid_data, feature_columns)\n",
    "    test_data = true_rank_gaus_scale(test_data, feature_columns)\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "    \n",
    "\n",
    "def quantile_scaling(train_data, valid_data, test_data, feature_columns):\n",
    "    global RANDOM_SEED\n",
    "    \n",
    "    scaler = QuantileTransformer(n_quantiles=100,random_state=RANDOM_SEED, output_distribution=\"normal\")\n",
    "    train_data[feature_columns] = scaler.fit_transform(train_data[feature_columns])\n",
    "    valid_data[feature_columns] = scaler.transform(valid_data[feature_columns])\n",
    "    test_data[feature_columns] = scaler.transform(test_data[feature_columns])\n",
    "    \n",
    "    return train_data, valid_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032477,
     "end_time": "2020-12-30T11:57:05.292865",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.260388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:05.371790Z",
     "iopub.status.busy": "2020-12-30T11:57:05.370648Z",
     "iopub.status.idle": "2020-12-30T11:57:05.373902Z",
     "shell.execute_reply": "2020-12-30T11:57:05.373316Z"
    },
    "papermill": {
     "duration": 0.047876,
     "end_time": "2020-12-30T11:57:05.373991",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.326115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloader(data, batch_size, shuffle, target_columns=None):\n",
    "    gene_features = [c for c in data.columns if 'g-' in c]\n",
    "    cell_features = [c for c in data.columns if 'c-' in c]\n",
    "    \n",
    "    dataset = MoaDataset(data, gene_features, cell_features, target_columns)\n",
    "    \n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=shuffle)\n",
    "\n",
    "def create_meta_dataloader(data, batch_size, shuffle, target_columns=None):\n",
    "    global meta_feature_columns\n",
    "    \n",
    "    dataset = MoaMetaDataset(data, feature_ids=meta_feature_columns, target_ids=target_columns)\n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       batch_size=batch_size,                                    \n",
    "                                       shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:05.462616Z",
     "iopub.status.busy": "2020-12-30T11:57:05.460920Z",
     "iopub.status.idle": "2020-12-30T11:57:05.463413Z",
     "shell.execute_reply": "2020-12-30T11:57:05.463943Z"
    },
    "papermill": {
     "duration": 0.058395,
     "end_time": "2020-12-30T11:57:05.464054",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.405659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pca_transform(fitted_pca, data, feature_columns, sig_ids, base_feature_name):\n",
    "    feature_data = fitted_pca.transform(data[feature_columns].values)\n",
    "    df = pd.DataFrame(feature_data, columns =[f'{base_feature_name}-{i}' for i in range(feature_data.shape[1])])\n",
    "    df['sig_id'] = sig_ids\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_fold_data(train_data, test_data, fold, dataloader_factory_func, scaling_func=None, add_pca=False):\n",
    "    global feature_columns, target_columns, gene_features, cell_features\n",
    "    \n",
    "    fold_train_data = train_data[train_data.fold != fold].reset_index(drop=True)\n",
    "    fold_valid_data = train_data[train_data.fold == fold].reset_index(drop=True)\n",
    "    \n",
    "    if add_pca:\n",
    "        fold_data = [fold_train_data, fold_valid_data, test_data]\n",
    "        \n",
    "        pca_genes = PCA(n_components=150)\n",
    "        pca_cells = PCA(n_components=30)\n",
    "        \n",
    "        pca_genes.fit(fold_train_data[gene_features].values)\n",
    "        pca_cells.fit(fold_train_data[cell_features].values)\n",
    "        \n",
    "        for fitted_pca, pca_features, colum_name in [(pca_genes, gene_features, 'g-pca'), (pca_cells, cell_features, 'c-pca')]:\n",
    "            for i, pca_data in enumerate(fold_data):\n",
    "                fitted_pca_data = pca_transform(fitted_pca=fitted_pca, \n",
    "                                                data=pca_data, \n",
    "                                                feature_columns=pca_features, \n",
    "                                                sig_ids=pca_data.sig_id.values, \n",
    "                                                base_feature_name=colum_name)\n",
    "                fold_data[i] = pd.merge(fold_data[i], fitted_pca_data, on='sig_id')\n",
    "            \n",
    "           \n",
    "        fold_train_data = fold_data[0]\n",
    "        fold_valid_data = fold_data[1]\n",
    "        test_data = fold_data[2]\n",
    "    \n",
    "    if scaling_func is not None:\n",
    "        fold_train_data, fold_valid_data, test_data = scaling_func(fold_train_data, fold_valid_data, test_data, feature_columns)\n",
    "      \n",
    "    train_dataloader = dataloader_factory_func(data=fold_train_data, batch_size=BATCH_SIZE, shuffle=True, target_columns=target_columns)\n",
    "    valid_dataloader = dataloader_factory_func(data=fold_valid_data, batch_size=BATCH_SIZE, shuffle=False, target_columns=target_columns)\n",
    "    test_dataloader = dataloader_factory_func(data=test_data, batch_size=BATCH_SIZE, shuffle=False, target_columns=None)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035141,
     "end_time": "2020-12-30T11:57:05.531623",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.496482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Blending functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:05.621553Z",
     "iopub.status.busy": "2020-12-30T11:57:05.619800Z",
     "iopub.status.idle": "2020-12-30T11:57:05.622241Z",
     "shell.execute_reply": "2020-12-30T11:57:05.622877Z"
    },
    "papermill": {
     "duration": 0.058766,
     "end_time": "2020-12-30T11:57:05.623000",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.564234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_numpy(y_pred):\n",
    "    loss = 0\n",
    "    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n",
    "    return loss / y_pred.shape[1]\n",
    "\n",
    "def func_numpy_metric(weights):\n",
    "    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n",
    "    score = log_loss_numpy(oof_blend)\n",
    "    \n",
    "    coef = 1e-6\n",
    "    penalty = coef * (np.sum(weights) - 1) ** 2\n",
    "    return score + penalty\n",
    "\n",
    "def grad_func(weights):\n",
    "    oof_clip = np.clip(oof, 1e-15, 1 - 1e-15)\n",
    "    gradients = np.zeros(oof.shape[0])\n",
    "    for i in range(oof.shape[0]):\n",
    "        a, b, c = y_true, oof_clip[i], 0\n",
    "        for j in range(oof.shape[0]):\n",
    "            if j != i:\n",
    "                c += weights[j] * oof_clip[j]\n",
    "        gradients[i] = -np.mean((-a*b+(b**2)*weights[i]+b*c)/((b**2)*(weights[i]**2)+2*b*c*weights[i]-b*weights[i]+(c**2)-c))\n",
    "    return gradients\n",
    "\n",
    "oof = []\n",
    "y_true = []\n",
    "def find_optimal_blend(predictions, train_data, target_columns):\n",
    "    \n",
    "    global oof, y_true\n",
    "    y_true = train_data.sort_values(by='sig_id')[target_columns].values\n",
    "    oof = np.zeros((len(predictions), y_true.shape[0], y_true.shape[1]))\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        oof[i] = pred.sort_values(by='sig_id')[target_columns].values\n",
    "\n",
    "    tol = 1e-10\n",
    "    init_guess = [1 / oof.shape[0]] * oof.shape[0]\n",
    "    bnds = [(0, 1) for _ in range(oof.shape[0])]\n",
    "    cons = {'type': 'eq', \n",
    "            'fun': lambda x: np.sum(x) - 1, \n",
    "            'jac': lambda x: [1] * len(x)}\n",
    "\n",
    "    res_scipy = minimize(fun = func_numpy_metric, \n",
    "                         x0 = init_guess, \n",
    "                         method = 'SLSQP', \n",
    "                         jac = grad_func, \n",
    "                         bounds = bnds, \n",
    "                         constraints = cons, \n",
    "                         tol = tol)\n",
    "    \n",
    "    return res_scipy.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031904,
     "end_time": "2020-12-30T11:57:05.687635",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.655731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:05.761869Z",
     "iopub.status.busy": "2020-12-30T11:57:05.761023Z",
     "iopub.status.idle": "2020-12-30T11:57:05.764361Z",
     "shell.execute_reply": "2020-12-30T11:57:05.763838Z"
    },
    "papermill": {
     "duration": 0.044604,
     "end_time": "2020-12-30T11:57:05.764462",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.719858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(model, data_loader, target_columns):\n",
    "    predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch_predictions = model(batch).sigmoid().detach().cpu().numpy()\n",
    "        sig_ids = np.array(batch['sig_id'])\n",
    "\n",
    "        df = pd.DataFrame(batch_predictions, columns=target_columns)\n",
    "        df['sig_id'] = sig_ids\n",
    "        predictions.append(df)\n",
    "\n",
    "    return pd.concat(predictions).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:05.847298Z",
     "iopub.status.busy": "2020-12-30T11:57:05.845925Z",
     "iopub.status.idle": "2020-12-30T11:57:05.849017Z",
     "shell.execute_reply": "2020-12-30T11:57:05.848487Z"
    },
    "papermill": {
     "duration": 0.052229,
     "end_time": "2020-12-30T11:57:05.849110",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.796881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_log_loss(predicted_df, train_df, target_columns):\n",
    "    predicted_df = predicted_df.copy()\n",
    "    train_df = train_df.copy()\n",
    "    \n",
    "    predicted_df = predicted_df[target_columns + ['sig_id']].reset_index(drop=True)\n",
    "    predicted_df = predicted_df.sort_values(by=['sig_id'])\n",
    "    predicted_df = predicted_df.drop('sig_id', axis=1)\n",
    "\n",
    "    true_df = train_df[target_columns + ['sig_id']].reset_index(drop=True)\n",
    "    true_df = true_df.sort_values(by=['sig_id'])\n",
    "    true_df = true_df.drop('sig_id', axis=1)\n",
    "\n",
    "    predicted_values = predicted_df.values\n",
    "    true_values = true_df.values\n",
    "    \n",
    "    score = 0\n",
    "    loss_per_class = []\n",
    "    for i in range(predicted_values.shape[1]):\n",
    "        _score = log_loss(true_values[:, i].astype(np.float), predicted_values[:, i].astype(np.float), eps=1e-15, labels=[1,0])\n",
    "        loss_per_class.append(_score)\n",
    "        score += _score / predicted_values.shape[1]\n",
    "\n",
    "    return score, loss_per_class\n",
    "\n",
    "def scale_predictions(predictions, target_columns, scale_values=None):\n",
    "    predictions = [p.copy() for p in predictions]\n",
    "    predictions = [p.sort_values(by=['sig_id']).reset_index(drop=True) for p in predictions]\n",
    "    \n",
    "    final_predictions = np.zeros((predictions[0].shape[0], len(target_columns)))\n",
    "    \n",
    "    for i, p in enumerate(predictions):\n",
    "        p_values = p[target_columns].values\n",
    "        \n",
    "        if scale_values is None:\n",
    "            final_predictions += p_values / len(predictions)\n",
    "        else:\n",
    "            final_predictions += (p_values * scale_values[i])\n",
    "        \n",
    "    predictions_df = predictions[0].copy()\n",
    "    predictions_df.loc[:, target_columns] = final_predictions\n",
    "    \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:05.932584Z",
     "iopub.status.busy": "2020-12-30T11:57:05.922175Z",
     "iopub.status.idle": "2020-12-30T11:57:05.941571Z",
     "shell.execute_reply": "2020-12-30T11:57:05.941091Z"
    },
    "papermill": {
     "duration": 0.059861,
     "end_time": "2020-12-30T11:57:05.941663",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.881802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainFactory:\n",
    "    \n",
    "    @classmethod\n",
    "    def model_version1(cls, train_loader, epochs):\n",
    "        global model_config, DEVICE\n",
    "        \n",
    "        model = MoaModel_V1(model_config).to(DEVICE)\n",
    "        best_model = MoaModel_V1(model_config).to(DEVICE)\n",
    "                \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.04647353847564317, weight_decay=8.087569236449597e-06)\n",
    "\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader)*epochs//2, num_training_steps=len(train_loader)*epochs)\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        return model, best_model, optimizer, scheduler, loss_fn\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def model_version2(cls, train_loader, epochs):\n",
    "        global model_config, DEVICE\n",
    "        \n",
    "        model = MoaModel_V2(model_config).to(DEVICE)\n",
    "        best_model = MoaModel_V2(model_config).to(DEVICE)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                     lr=1e-3,\n",
    "                                     weight_decay=1e-5)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "                                                        max_lr=1e-2,\n",
    "                                                        epochs=epochs, \n",
    "                                                        steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        return model, best_model, optimizer, scheduler, loss_fn\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def model_version3(cls, train_loader, epochs):\n",
    "        global model_config, DEVICE\n",
    "        \n",
    "        model = MoaModel_V3(model_config).to(DEVICE)\n",
    "        best_model = MoaModel_V3(model_config).to(DEVICE)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                     lr=1e-3,\n",
    "                                     weight_decay=1e-5)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "                                                        max_lr=1e-2,\n",
    "                                                        epochs=epochs, \n",
    "                                                        steps_per_epoch=len(train_loader))\n",
    "\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        return model, best_model, optimizer, scheduler, loss_fn\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def model_version4(cls, train_loader, epochs):\n",
    "        global model_config, DEVICE\n",
    "        \n",
    "        model = MoaModel_V4(model_config).to(DEVICE)\n",
    "        best_model = MoaModel_V4(model_config).to(DEVICE)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                     lr=1e-3,\n",
    "                                     weight_decay=1e-5)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "                                                        max_lr=5e-3,\n",
    "                                                        epochs=epochs, \n",
    "                                                        steps_per_epoch=len(train_loader))\n",
    "\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        return model, best_model, optimizer, scheduler, loss_fn\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def meta_model(cls, train_loader, epochs):\n",
    "        global model_config, DEVICE\n",
    "        \n",
    "        model = MetaModel(model_config).to(DEVICE)\n",
    "        best_model = MetaModel(model_config).to(DEVICE)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                     lr=1e-3,\n",
    "                                     weight_decay=1e-5)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "                                                        max_lr=1e-2,\n",
    "                                                        epochs=epochs, \n",
    "                                                        steps_per_epoch=len(train_loader))\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        return model, best_model, optimizer, scheduler, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:06.019766Z",
     "iopub.status.busy": "2020-12-30T11:57:06.019044Z",
     "iopub.status.idle": "2020-12-30T11:57:06.023052Z",
     "shell.execute_reply": "2020-12-30T11:57:06.022585Z"
    },
    "papermill": {
     "duration": 0.048705,
     "end_time": "2020-12-30T11:57:06.023143",
     "exception": false,
     "start_time": "2020-12-30T11:57:05.974438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, best_model, optimizer, scheduler, loss_fn, train_loader, valid_loader, test_loader, epochs):\n",
    "    global gene_features, cell_features, target_columns\n",
    "    \n",
    "    train_data = train_loader.dataset.dataset_df\n",
    "    valid_data = valid_loader.dataset.dataset_df\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for train_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = model(train_batch)\n",
    "            y_true = train_batch['y'].to(DEVICE)\n",
    "            \n",
    "            curr_train_loss = loss_fn(y_pred, y_true)\n",
    "            \n",
    "            curr_train_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += ( curr_train_loss.item() * (len(train_batch['sig_id']) / len(train_data)))\n",
    "            \n",
    "            \n",
    "        valid_predictions = inference(model, valid_loader, target_columns)\n",
    "        valid_loss, _ = calculate_log_loss(valid_predictions, valid_data, target_columns)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "                           \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch:{epoch} \\t train_loss:{train_loss:.10f} \\t valid_loss:{valid_loss:.10f}')\n",
    "            \n",
    "    \n",
    "    valid_predictions = inference(best_model, valid_loader, target_columns)\n",
    "    test_predictions = inference(best_model, test_loader, target_columns)\n",
    "    \n",
    "    return best_model, valid_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:06.101858Z",
     "iopub.status.busy": "2020-12-30T11:57:06.100046Z",
     "iopub.status.idle": "2020-12-30T11:57:06.102540Z",
     "shell.execute_reply": "2020-12-30T11:57:06.103005Z"
    },
    "papermill": {
     "duration": 0.043468,
     "end_time": "2020-12-30T11:57:06.103111",
     "exception": false,
     "start_time": "2020-12-30T11:57:06.059643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "FOLDS = 5\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4092\n",
    "SEEDS = [11, 221, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:06.181544Z",
     "iopub.status.busy": "2020-12-30T11:57:06.180598Z",
     "iopub.status.idle": "2020-12-30T11:57:06.919535Z",
     "shell.execute_reply": "2020-12-30T11:57:06.918122Z"
    },
    "papermill": {
     "duration": 0.783133,
     "end_time": "2020-12-30T11:57:06.919663",
     "exception": false,
     "start_time": "2020-12-30T11:57:06.136530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Creating the cross validation strategy\n",
    "train_data = create_cross_validation_strategy(train_data, target_columns, FOLDS, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:06.998526Z",
     "iopub.status.busy": "2020-12-30T11:57:06.997516Z",
     "iopub.status.idle": "2020-12-30T11:57:07.139543Z",
     "shell.execute_reply": "2020-12-30T11:57:07.138961Z"
    },
    "papermill": {
     "duration": 0.184977,
     "end_time": "2020-12-30T11:57:07.139655",
     "exception": false,
     "start_time": "2020-12-30T11:57:06.954678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = train_data[train_data.cp_type == 'trt_cp'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:07.215380Z",
     "iopub.status.busy": "2020-12-30T11:57:07.214752Z",
     "iopub.status.idle": "2020-12-30T11:57:07.219113Z",
     "shell.execute_reply": "2020-12-30T11:57:07.218616Z"
    },
    "papermill": {
     "duration": 0.043584,
     "end_time": "2020-12-30T11:57:07.219227",
     "exception": false,
     "start_time": "2020-12-30T11:57:07.175643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelTrainConfig:\n",
    "    def __init__(self, model_name, factory_func, scaling_func, add_pca):\n",
    "        self.model_name = model_name\n",
    "        self.factory_func = factory_func\n",
    "        self.scaling_func = scaling_func\n",
    "        self.add_pca = add_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:07.299025Z",
     "iopub.status.busy": "2020-12-30T11:57:07.297281Z",
     "iopub.status.idle": "2020-12-30T11:57:07.299750Z",
     "shell.execute_reply": "2020-12-30T11:57:07.300216Z"
    },
    "papermill": {
     "duration": 0.046335,
     "end_time": "2020-12-30T11:57:07.300325",
     "exception": false,
     "start_time": "2020-12-30T11:57:07.253990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version1 = ModelTrainConfig(model_name='version_1', \n",
    "                                  factory_func=TrainFactory.model_version1, \n",
    "                                  scaling_func=true_rankgaus_scaling,\n",
    "                                  add_pca=False)\n",
    "\n",
    "model_version2 = ModelTrainConfig(model_name='version_2', \n",
    "                                  factory_func=TrainFactory.model_version2, \n",
    "                                  scaling_func=quantile_dosetime_scaling,\n",
    "                                  add_pca=False)\n",
    "\n",
    "model_version3 = ModelTrainConfig(model_name='version_3', \n",
    "                                  factory_func=TrainFactory.model_version3, \n",
    "                                  scaling_func=quantile_scaling,\n",
    "                                  add_pca=False)\n",
    "\n",
    "model_version4 = ModelTrainConfig(model_name='version_4', \n",
    "                                  factory_func=TrainFactory.model_version4, \n",
    "                                  scaling_func=quantile_scaling,\n",
    "                                  add_pca=False)\n",
    "\n",
    "\n",
    "meta_model = ModelTrainConfig(model_name='meta_model', \n",
    "                              factory_func=TrainFactory.meta_model, \n",
    "                              scaling_func=None,\n",
    "                              add_pca=False)\n",
    "\n",
    "\n",
    "models_train_configs = [model_version1, model_version2, model_version3, model_version4]\n",
    "\n",
    "meta_models_train_configs = [meta_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T11:57:07.386154Z",
     "iopub.status.busy": "2020-12-30T11:57:07.381857Z",
     "iopub.status.idle": "2020-12-30T12:13:55.070076Z",
     "shell.execute_reply": "2020-12-30T12:13:55.070704Z"
    },
    "papermill": {
     "duration": 1007.736072,
     "end_time": "2020-12-30T12:13:55.070894",
     "exception": false,
     "start_time": "2020-12-30T11:57:07.334822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:version_1\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_1 \t Seed:11 \t oof_loss:0.0172470141\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_1 \t Seed:221 \t oof_loss:0.0172696376\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_1 \t Seed:50 \t oof_loss:0.0172261048\n",
      "Model:version_1 \t valid_loss:0.0171078454\n",
      "Training model:version_2\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_2 \t Seed:11 \t oof_loss:0.0172131784\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_2 \t Seed:221 \t oof_loss:0.0172033214\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_2 \t Seed:50 \t oof_loss:0.0171891465\n",
      "Model:version_2 \t valid_loss:0.0170618974\n",
      "Training model:version_3\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_3 \t Seed:11 \t oof_loss:0.0173302844\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_3 \t Seed:221 \t oof_loss:0.0173199846\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_3 \t Seed:50 \t oof_loss:0.0173412495\n",
      "Model:version_3 \t valid_loss:0.0171357658\n",
      "Training model:version_4\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_4 \t Seed:11 \t oof_loss:0.0171740872\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_4 \t Seed:221 \t oof_loss:0.0172040589\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:version_4 \t Seed:50 \t oof_loss:0.0171844925\n",
      "Model:version_4 \t valid_loss:0.0169953153\n"
     ]
    }
   ],
   "source": [
    "models_valid_predictions = []\n",
    "models_test_predictions = []\n",
    "\n",
    "seed_losses = []\n",
    "\n",
    "for model_train_config in models_train_configs:\n",
    "    print(f'Training model:{model_train_config.model_name}')\n",
    "    \n",
    "    single_model_valid_predictions = []\n",
    "    single_model_test_predictions = []\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        seed_everything(seed)\n",
    "\n",
    "        model_seed_valid_predictions = []\n",
    "        model_seed_test_predictions = []\n",
    "\n",
    "        for fold in range(FOLDS):\n",
    "            print(f'Training fold: {fold}')\n",
    "\n",
    "            fold_test_data = test_data[test_data.cp_type == 'trt_cp'].reset_index(drop=True)\n",
    "\n",
    "            train_loader, valid_loader, test_loader = preprocess_fold_data(train_data=train_data, \n",
    "                                                                           test_data=fold_test_data, \n",
    "                                                                           fold=fold, \n",
    "                                                                           dataloader_factory_func=create_dataloader,\n",
    "                                                                           scaling_func=model_train_config.scaling_func)\n",
    "            \n",
    "            number_of_genes, number_of_cells = train_loader.dataset.number_of_features()\n",
    "\n",
    "            model_config = ModelConfig(number_of_features=number_of_genes + number_of_cells,\n",
    "                                       number_of_genes=number_of_genes, \n",
    "                                       number_of_cells=number_of_cells, \n",
    "                                       number_of_targets=len(target_columns))\n",
    "\n",
    "            model, _, optimizer, scheduler, loss_fn = model_train_config.factory_func(train_loader, EPOCHS)\n",
    "            \n",
    "            model.load_state_dict(torch.load(f'../input/moablogdataset/model-{model_train_config.model_name}_fold-{fold}_seed-{seed}', \n",
    "                                             map_location=torch.device(DEVICE)))\n",
    "            \n",
    "            valid_predictions = inference(model, valid_loader, target_columns)\n",
    "            test_predictions = inference(model, test_loader, target_columns)\n",
    "\n",
    "            model_seed_valid_predictions.append(valid_predictions)\n",
    "            model_seed_test_predictions.append(test_predictions)\n",
    "            print('-' * 100)\n",
    "\n",
    "\n",
    "        valid_predictions = pd.concat(model_seed_valid_predictions).reset_index(drop=True)\n",
    "        test_predictions = scale_predictions(model_seed_test_predictions, target_columns)\n",
    "        \n",
    "        single_model_valid_predictions.append(valid_predictions)\n",
    "        single_model_test_predictions.append(test_predictions)\n",
    "        \n",
    "        valid_loss, _ = calculate_log_loss(valid_predictions, train_data, target_columns)\n",
    "\n",
    "        seed_losses.append(valid_loss)\n",
    "\n",
    "        print(f'Model:{model_train_config.model_name} \\t Seed:{seed} \\t oof_loss:{valid_loss:.10f}')\n",
    "\n",
    "    valid_predictions = scale_predictions(single_model_valid_predictions, target_columns)\n",
    "    test_predictions = scale_predictions(single_model_test_predictions, target_columns)\n",
    "    \n",
    "    models_valid_predictions.append(valid_predictions)\n",
    "    models_test_predictions.append(test_predictions)\n",
    "    \n",
    "    valid_loss, _ = calculate_log_loss(valid_predictions, train_data, target_columns)\n",
    "\n",
    "    print(f'Model:{model_train_config.model_name} \\t valid_loss:{valid_loss:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:13:55.201061Z",
     "iopub.status.busy": "2020-12-30T12:13:55.200152Z",
     "iopub.status.idle": "2020-12-30T12:14:06.586463Z",
     "shell.execute_reply": "2020-12-30T12:14:06.585838Z"
    },
    "papermill": {
     "duration": 11.454896,
     "end_time": "2020-12-30T12:14:06.586578",
     "exception": false,
     "start_time": "2020-12-30T12:13:55.131682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal blend weights: [0.24941872 0.21155637 0.14350502 0.39551989]\n"
     ]
    }
   ],
   "source": [
    "#Finding optimal blend weights\n",
    "blend_weights = find_optimal_blend(models_valid_predictions, train_data, target_columns)\n",
    "\n",
    "print(f'Optimal blend weights: {blend_weights}')\n",
    "\n",
    "level1_valid_predictions = scale_predictions(models_valid_predictions, target_columns, blend_weights)\n",
    "level1_test_predictions = scale_predictions(models_test_predictions, target_columns, blend_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:06.719719Z",
     "iopub.status.busy": "2020-12-30T12:14:06.718410Z",
     "iopub.status.idle": "2020-12-30T12:14:07.478048Z",
     "shell.execute_reply": "2020-12-30T12:14:07.477483Z"
    },
    "papermill": {
     "duration": 0.831177,
     "end_time": "2020-12-30T12:14:07.478153",
     "exception": false,
     "start_time": "2020-12-30T12:14:06.646976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta_features = pd.DataFrame(data=train_data.sig_id.values, columns=['sig_id'])\n",
    "\n",
    "for version, valid_predictions in enumerate(models_valid_predictions):\n",
    "    df = valid_predictions.rename(columns={v:f'meta-f-{i}-model{version + 1}' if v != 'sig_id' else v for i, v in enumerate(valid_predictions.columns)})\n",
    "    meta_features = pd.merge(meta_features, df, on='sig_id')\n",
    "    \n",
    "train_data = pd.merge(train_data, meta_features, on='sig_id')\n",
    "\n",
    "\n",
    "meta_features = pd.DataFrame(data=test_data.sig_id.values, columns=['sig_id'])\n",
    "\n",
    "for version, test_predictions in enumerate(models_test_predictions):\n",
    "    df = test_predictions.rename(columns={v:f'meta-f-{i}-model{version + 1}' if v != 'sig_id' else v for i, v in enumerate(test_predictions.columns)})\n",
    "    meta_features = pd.merge(meta_features, df, on='sig_id')\n",
    "    \n",
    "test_data = pd.merge(test_data, meta_features, on='sig_id')\n",
    "\n",
    "meta_feature_columns = [c for c in train_data.columns if 'meta-f-' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:07.622794Z",
     "iopub.status.busy": "2020-12-30T12:14:07.620431Z",
     "iopub.status.idle": "2020-12-30T12:14:27.605213Z",
     "shell.execute_reply": "2020-12-30T12:14:27.605953Z"
    },
    "papermill": {
     "duration": 20.06772,
     "end_time": "2020-12-30T12:14:27.606135",
     "exception": false,
     "start_time": "2020-12-30T12:14:07.538415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:meta_model\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:meta_model \t Seed:11 \t oof_loss:0.0173538035\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:meta_model \t Seed:221 \t oof_loss:0.0173678223\n",
      "Training fold: 0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold: 4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model:meta_model \t Seed:50 \t oof_loss:0.0173974004\n",
      "Model:meta_model \t valid_loss:0.0172423408\n"
     ]
    }
   ],
   "source": [
    "models_valid_predictions = []\n",
    "models_test_predictions = []\n",
    "\n",
    "seed_losses = []\n",
    "\n",
    "for model_train_config in meta_models_train_configs:\n",
    "    print(f'Training model:{model_train_config.model_name}')\n",
    "    \n",
    "    single_model_valid_predictions = []\n",
    "    single_model_test_predictions = []\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        seed_everything(seed)\n",
    "\n",
    "        model_seed_valid_predictions = []\n",
    "        model_seed_test_predictions = []\n",
    "\n",
    "        for fold in range(FOLDS):\n",
    "            print(f'Training fold: {fold}')\n",
    "\n",
    "            fold_test_data = test_data[test_data.cp_type == 'trt_cp'].reset_index(drop=True)\n",
    "\n",
    "            train_loader, valid_loader, test_loader = preprocess_fold_data(train_data=train_data, \n",
    "                                                                           test_data=fold_test_data,\n",
    "                                                                           fold=fold, \n",
    "                                                                           dataloader_factory_func=create_meta_dataloader,\n",
    "                                                                           scaling_func=model_train_config.scaling_func)\n",
    "            \n",
    "            model_config = ModelConfig(number_of_features=len(feature_columns),\n",
    "                                       number_of_genes=0, \n",
    "                                       number_of_cells=0, \n",
    "                                       number_of_targets=len(target_columns))\n",
    "\n",
    "            model, _, optimizer, scheduler, loss_fn = model_train_config.factory_func(train_loader, EPOCHS)\n",
    "            \n",
    "            model.load_state_dict(torch.load(f'../input/moablognotebook-stacking/model-{model_train_config.model_name}_fold-{fold}_seed-{seed}', \n",
    "                                             map_location=torch.device(DEVICE)))\n",
    "            \n",
    "            valid_predictions = inference(model, valid_loader, target_columns)\n",
    "            test_predictions = inference(model, test_loader, target_columns)\n",
    "\n",
    "            model_seed_valid_predictions.append(valid_predictions)\n",
    "            model_seed_test_predictions.append(test_predictions)\n",
    "            print('-' * 100)\n",
    "\n",
    "\n",
    "        valid_predictions = pd.concat(model_seed_valid_predictions).reset_index(drop=True)\n",
    "        test_predictions = scale_predictions(model_seed_test_predictions, target_columns)\n",
    "        \n",
    "        single_model_valid_predictions.append(valid_predictions)\n",
    "        single_model_test_predictions.append(test_predictions)\n",
    "        \n",
    "        valid_loss, _ = calculate_log_loss(valid_predictions, train_data, target_columns)\n",
    "\n",
    "        seed_losses.append(valid_loss)\n",
    "\n",
    "        print(f'Model:{model_train_config.model_name} \\t Seed:{seed} \\t oof_loss:{valid_loss:.10f}')\n",
    "\n",
    "    valid_predictions = scale_predictions(single_model_valid_predictions, target_columns)\n",
    "    test_predictions = scale_predictions(single_model_test_predictions, target_columns)\n",
    "    \n",
    "    models_valid_predictions.append(valid_predictions)\n",
    "    models_test_predictions.append(test_predictions)\n",
    "    \n",
    "    valid_loss, _ = calculate_log_loss(valid_predictions, train_data, target_columns)\n",
    "\n",
    "    print(f'Model:{model_train_config.model_name} \\t valid_loss:{valid_loss:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:27.763030Z",
     "iopub.status.busy": "2020-12-30T12:14:27.747607Z",
     "iopub.status.idle": "2020-12-30T12:14:30.327654Z",
     "shell.execute_reply": "2020-12-30T12:14:30.327039Z"
    },
    "papermill": {
     "duration": 2.653747,
     "end_time": "2020-12-30T12:14:30.327769",
     "exception": false,
     "start_time": "2020-12-30T12:14:27.674022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal blend weights: [1.]\n"
     ]
    }
   ],
   "source": [
    "blend_weights = find_optimal_blend(models_valid_predictions, train_data, target_columns)\n",
    "\n",
    "print(f'Optimal blend weights: {blend_weights}')\n",
    "\n",
    "level2_valid_predictions = scale_predictions(models_valid_predictions, target_columns, blend_weights)\n",
    "level2_test_predictions = scale_predictions(models_test_predictions, target_columns, blend_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:30.484727Z",
     "iopub.status.busy": "2020-12-30T12:14:30.469646Z",
     "iopub.status.idle": "2020-12-30T12:14:38.057571Z",
     "shell.execute_reply": "2020-12-30T12:14:38.056926Z"
    },
    "papermill": {
     "duration": 7.662444,
     "end_time": "2020-12-30T12:14:38.057695",
     "exception": false,
     "start_time": "2020-12-30T12:14:30.395251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal blend weights: [0.71978409 0.28021591]\n"
     ]
    }
   ],
   "source": [
    "combined_models_valid = [level1_valid_predictions, level2_valid_predictions]\n",
    "combined_models_test = [level1_test_predictions, level2_test_predictions]\n",
    "\n",
    "#Finding optimal blend weights\n",
    "blend_weights = find_optimal_blend(combined_models_valid, train_data, target_columns)\n",
    "\n",
    "print(f'Optimal blend weights: {blend_weights}')\n",
    "\n",
    "valid_predictions = scale_predictions(combined_models_valid, target_columns, blend_weights)\n",
    "test_predictions = scale_predictions(combined_models_test, target_columns, blend_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:38.220054Z",
     "iopub.status.busy": "2020-12-30T12:14:38.219140Z",
     "iopub.status.idle": "2020-12-30T12:14:38.222981Z",
     "shell.execute_reply": "2020-12-30T12:14:38.227093Z"
    },
    "papermill": {
     "duration": 0.095447,
     "end_time": "2020-12-30T12:14:38.227270",
     "exception": false,
     "start_time": "2020-12-30T12:14:38.131823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, model_config in enumerate(models_train_configs):\n",
    "#     models_valid_predictions[i].to_csv(f'{model_config.model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:38.416019Z",
     "iopub.status.busy": "2020-12-30T12:14:38.414662Z",
     "iopub.status.idle": "2020-12-30T12:14:39.465673Z",
     "shell.execute_reply": "2020-12-30T12:14:39.464992Z"
    },
    "papermill": {
     "duration": 1.142121,
     "end_time": "2020-12-30T12:14:39.465816",
     "exception": false,
     "start_time": "2020-12-30T12:14:38.323695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.016876570376204887\n"
     ]
    }
   ],
   "source": [
    "validation_loss, _ = calculate_log_loss(valid_predictions, train_data, target_columns)\n",
    "print(f'Validation loss: {validation_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:39.625184Z",
     "iopub.status.busy": "2020-12-30T12:14:39.624389Z",
     "iopub.status.idle": "2020-12-30T12:14:40.472955Z",
     "shell.execute_reply": "2020-12-30T12:14:40.474089Z"
    },
    "papermill": {
     "duration": 0.932086,
     "end_time": "2020-12-30T12:14:40.474269",
     "exception": false,
     "start_time": "2020-12-30T12:14:39.542183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "\n",
    "zero_ids = test_data[test_data.cp_type == 'ctl_vehicle'].sig_id.values\n",
    "\n",
    "zero_df = pd.DataFrame(np.zeros((len(zero_ids), len(target_columns))), columns=target_columns)\n",
    "zero_df['sig_id'] = zero_ids\n",
    "\n",
    "nonzero_df = test_predictions[~test_predictions.sig_id.isin(zero_ids)]\n",
    "nonzero_df = nonzero_df[target_columns + ['sig_id']].reset_index(drop=True)\n",
    "submission = pd.concat([nonzero_df, zero_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:40.701123Z",
     "iopub.status.busy": "2020-12-30T12:14:40.699998Z",
     "iopub.status.idle": "2020-12-30T12:14:40.720971Z",
     "shell.execute_reply": "2020-12-30T12:14:40.720258Z"
    },
    "papermill": {
     "duration": 0.146346,
     "end_time": "2020-12-30T12:14:40.721113",
     "exception": false,
     "start_time": "2020-12-30T12:14:40.574767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3982, 876)\n",
      "(3624, 207)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "print(test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:41.031935Z",
     "iopub.status.busy": "2020-12-30T12:14:41.031069Z",
     "iopub.status.idle": "2020-12-30T12:14:43.239187Z",
     "shell.execute_reply": "2020-12-30T12:14:43.240042Z"
    },
    "papermill": {
     "duration": 2.322637,
     "end_time": "2020-12-30T12:14:43.240183",
     "exception": false,
     "start_time": "2020-12-30T12:14:40.917546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-30T12:14:43.401560Z",
     "iopub.status.busy": "2020-12-30T12:14:43.400972Z",
     "iopub.status.idle": "2020-12-30T12:14:43.416837Z",
     "shell.execute_reply": "2020-12-30T12:14:43.417296Z"
    },
    "papermill": {
     "duration": 0.105286,
     "end_time": "2020-12-30T12:14:43.417432",
     "exception": false,
     "start_time": "2020-12-30T12:14:43.312146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>...</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "      <th>sig_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.014136</td>\n",
       "      <td>0.020252</td>\n",
       "      <td>0.004270</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>id_0004d9e33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>0.005628</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.017624</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>id_001897cda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>0.018787</td>\n",
       "      <td>0.004244</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>id_00276f245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>0.024175</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>id_0027f1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.022876</td>\n",
       "      <td>0.024968</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.010406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>id_006fc47b8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  acat_inhibitor  \\\n",
       "0                     0.000735                0.000761        0.002231   \n",
       "1                     0.000364                0.000590        0.000955   \n",
       "2                     0.000540                0.000652        0.001746   \n",
       "3                     0.001258                0.001078        0.001461   \n",
       "4                     0.000595                0.000614        0.001891   \n",
       "\n",
       "   acetylcholine_receptor_agonist  acetylcholine_receptor_antagonist  \\\n",
       "0                        0.014136                           0.020252   \n",
       "1                        0.001978                           0.001480   \n",
       "2                        0.014749                           0.018787   \n",
       "3                        0.016156                           0.024175   \n",
       "4                        0.022876                           0.024968   \n",
       "\n",
       "   acetylcholinesterase_inhibitor  adenosine_receptor_agonist  \\\n",
       "0                        0.004270                    0.001988   \n",
       "1                        0.001381                    0.003455   \n",
       "2                        0.004244                    0.002901   \n",
       "3                        0.004641                    0.003597   \n",
       "4                        0.004218                    0.003785   \n",
       "\n",
       "   adenosine_receptor_antagonist  adenylyl_cyclase_activator  \\\n",
       "0                       0.004903                    0.000193   \n",
       "1                       0.009831                    0.005628   \n",
       "2                       0.005075                    0.000212   \n",
       "3                       0.002598                    0.000284   \n",
       "4                       0.002667                    0.000218   \n",
       "\n",
       "   adrenergic_receptor_agonist  ...  trpv_agonist  trpv_antagonist  \\\n",
       "0                     0.008273  ...      0.001171         0.003388   \n",
       "1                     0.013080  ...      0.000969         0.002198   \n",
       "2                     0.008258  ...      0.001370         0.002503   \n",
       "3                     0.012805  ...      0.000616         0.003063   \n",
       "4                     0.010406  ...      0.000998         0.003150   \n",
       "\n",
       "   tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0           0.001237                   0.000963   \n",
       "1           0.000495                   0.011636   \n",
       "2           0.007073                   0.004783   \n",
       "3           0.001573                   0.001257   \n",
       "4           0.004264                   0.001453   \n",
       "\n",
       "   ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                               0.000468         0.001311   0.001981   \n",
       "1                               0.000305         0.017624   0.000823   \n",
       "2                               0.000380         0.001662   0.001980   \n",
       "3                               0.000489         0.001067   0.001839   \n",
       "4                               0.000387         0.000788   0.002344   \n",
       "\n",
       "   vitamin_d_receptor_agonist  wnt_inhibitor        sig_id  \n",
       "0                    0.002160       0.001561  id_0004d9e33  \n",
       "1                    0.001873       0.003286  id_001897cda  \n",
       "2                    0.000780       0.001949  id_00276f245  \n",
       "3                    0.000412       0.001538  id_0027f1083  \n",
       "4                    0.000335       0.001417  id_006fc47b8  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1085.020876,
   "end_time": "2020-12-30T12:14:43.996215",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-30T11:56:38.975339",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
